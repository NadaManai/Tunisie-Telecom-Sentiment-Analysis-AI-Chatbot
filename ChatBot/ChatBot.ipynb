{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a91d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d8ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, intents, all_words, tags):\n",
    "        self.intents = intents\n",
    "        self.all_words = all_words\n",
    "        self.tags = tags\n",
    "        self.xy = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        xy = []\n",
    "        for intent in self.intents:\n",
    "            tag = intent['tag']\n",
    "            for pattern in intent['patterns']:\n",
    "                tokenized_sentence = tokenize(pattern)\n",
    "                bag = bag_of_words(tokenized_sentence, self.all_words)\n",
    "                label = self.tags.index(tag)\n",
    "                xy.append((bag, label))\n",
    "        return xy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bag, label = self.xy[index]\n",
    "        return bag, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout avec taux de 0.5\n",
    "\n",
    "        # Initialisation des poids avec la méthode de He\n",
    "        torch.nn.init.kaiming_uniform_(self.l1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la première couche cachée\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la deuxième couche cachée\n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa7c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2500], Loss: 0.0073, Accuracy: 92.46341463414635\n",
      "Epoch [200/2500], Loss: 0.0031, Accuracy: 95.79674796747967\n",
      "Epoch [300/2500], Loss: 0.0004, Accuracy: 96.92140921409214\n",
      "Epoch [400/2500], Loss: 0.0001, Accuracy: 97.44918699186992\n",
      "Epoch [500/2500], Loss: 0.0713, Accuracy: 97.80813008130082\n",
      "Epoch [600/2500], Loss: 0.0020, Accuracy: 98.02439024390245\n",
      "Epoch [700/2500], Loss: 0.0000, Accuracy: 98.18350754936121\n",
      "Epoch [800/2500], Loss: 0.0000, Accuracy: 98.3180894308943\n",
      "Epoch [900/2500], Loss: 0.0000, Accuracy: 98.41824751580849\n",
      "Epoch [1000/2500], Loss: 0.0000, Accuracy: 98.49512195121952\n",
      "Epoch [1100/2500], Loss: 0.0000, Accuracy: 98.54988913525499\n",
      "Epoch [1200/2500], Loss: 0.0000, Accuracy: 98.6050135501355\n",
      "Epoch [1300/2500], Loss: 0.0000, Accuracy: 98.64602876797998\n",
      "Epoch [1400/2500], Loss: 0.0000, Accuracy: 98.68524970963995\n",
      "Epoch [1500/2500], Loss: 0.0395, Accuracy: 98.71978319783197\n",
      "Epoch [1600/2500], Loss: 0.0000, Accuracy: 98.744918699187\n",
      "Epoch [1700/2500], Loss: 0.0000, Accuracy: 98.76948828311812\n",
      "Epoch [1800/2500], Loss: 0.0007, Accuracy: 98.79177958446252\n",
      "Epoch [1900/2500], Loss: 0.0000, Accuracy: 98.80958493795464\n",
      "Epoch [2000/2500], Loss: 0.0000, Accuracy: 98.82682926829268\n",
      "Epoch [2100/2500], Loss: 0.0000, Accuracy: 98.84010840108401\n",
      "Epoch [2200/2500], Loss: 0.0000, Accuracy: 98.85328898743533\n",
      "Epoch [2300/2500], Loss: 0.0000, Accuracy: 98.86284906327325\n",
      "Epoch [2400/2500], Loss: 0.0000, Accuracy: 98.87432249322494\n",
      "Epoch [2500/2500], Loss: 0.0000, Accuracy: 98.88162601626016\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_sentence = tokenize(pattern)\n",
    "        all_words.extend(tokenized_sentence)\n",
    "        xy.append((tokenized_sentence, tag))\n",
    "\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 512\n",
    "output_size = len(tags)\n",
    "\n",
    "dataset = ChatDataset(intents['intents'], all_words, tags)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "num_epochs = 2500\n",
    "correct=0\n",
    "total=0\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        accuracy =  100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a58e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to chatbot_model_base.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "    }\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76c397",
   "metadata": {},
   "source": [
    "## Deploying Q/A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c73379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ea958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset JSON\n",
    "with open('piaf-v1.1.json', 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Prétraitement des données\n",
    "paragraphs = []\n",
    "questions = []\n",
    "answers_start = []\n",
    "answers_end = []\n",
    "\n",
    "for article in dataset['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                answer_start = answer['answer_start']\n",
    "                answer_end = answer_start + len(answer['text'])\n",
    "\n",
    "                paragraphs.append(context)\n",
    "                questions.append(question)\n",
    "                answers_start.append(answer_start)\n",
    "                answers_end.append(answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74c291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle de question-réponse\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer une réponse à partir d'une question\n",
    "def generate_answer(question, context):\n",
    "    # Prétraitement des données\n",
    "    inputs = tokenizer_qa(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Prédiction de la réponse\n",
    "    outputs = model_qa(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Obtenir les indices de début et de fin de la réponse prédite\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    # Convertir les indices en texte\n",
    "    answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index+1]))\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f686d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour trouver le contexte le plus pertinent pour une question donnée\n",
    "def find_most_relevant_context(question, dataset):\n",
    "    # Utiliser TF-IDF pour représenter les contextes en vecteurs\n",
    "    contexts=[]\n",
    "    for article in dataset['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            contexts += [paragraph['context']]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(contexts)\n",
    "    \n",
    "    # Convertir la question en vecteur\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculer les similarités entre la question et les contextes\n",
    "    similarities = cosine_similarity(question_vector, vectors)\n",
    "    \n",
    "    # Trouver l'index du contexte le plus similaire\n",
    "    most_similar_index = similarities.argmax()\n",
    "    \n",
    "    # Renvoyer le contexte le plus similaire\n",
    "    return contexts[most_similar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05face",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8deac88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=146, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l3): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- IMPORT LIBRARY -----------------\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ------ IMPORT DATA & MODEL FOR BASE CASE ------\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdd2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(text):\n",
    "    cleaned_text = text.lower().strip() \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4614af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: blague\n",
      "QueryBot: J'ai mange une horloge hier, c'etait tres chronophage\n",
      "User: devinette\n",
      "QueryBot: Pourquoi un velo ne peut-il pas tenir tout seul ?.....Il est trop fatigue.\n",
      "User: bonjour\n",
      "QueryBot: Content de vous revoir\n",
      "User: ça va?\n",
      "QueryBot: Tout va bien.. Et vous?\n",
      "User: bien\n",
      "QueryBot: Content de le savoir!\n",
      "User:  Quelle est la dernière nouvelle sur Tunisie Télécom ?\n",
      "QueryBot: La dernière nouvelle sur Tunisie Télécom est : Tunisie Télécom et l’AtGmo : “Run of Heroes” pour soutenir les malades d’hémopathies\n",
      "User: Combien d'articles positifs y a-t-il en 2023 ?\n",
      "QueryBot: Il y a 9 articles positifs en 2023.\n",
      "User: Quelle est l'année avec le plus d'articles négatifs ?\n",
      "QueryBot: L'année avec le plus d'articles négatifs est 2019 avec 7 articles.\n",
      "User: Combien d'articles négatifs y a-t-il en 2024 ?\n",
      "QueryBot: Il y a 0 articles négatifs en 2024.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDébut de la conversation (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pour finir la conversation...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Saisie utilisateur\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "  \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "               \n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "               \n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "  \n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "  \n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        \n",
    "        if prob.item() > 0.75:  \n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                print(f\"{bot_name}: {response}\")\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b6f3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interface\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, Entry, END\n",
    "def send_message():\n",
    "    user_input = input_field.get()\n",
    "    input_field.delete(0, END)\n",
    "    chat_history.config(state=tk.NORMAL)\n",
    "    chat_history.insert(tk.END, f\"User: {user_input}\\n\", \"user\")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        root.quit()\n",
    "    elif \"tunisie télécom\" in user_input.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            response = f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\\n\"\n",
    "        else:\n",
    "            response = f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\\n\"\n",
    "    elif any(word in user_input.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', user_input)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article.get('SentimentCamembert', '')\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in user_input.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    response = f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\\n\"\n",
    "                else:\n",
    "                    response = f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\\n\"\n",
    "            elif \"combien d'articles négatifs\" in user_input.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    response = f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\\n\"\n",
    "                else:\n",
    "                    response = f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\\n\"\n",
    "            elif \"année avec le plus d'articles négatifs\" in user_input.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                response = f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\\n\"\n",
    "            elif \"année avec le plus d'articles positifs\" in user_input.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                response = f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\\n\"\n",
    "        else:\n",
    "            response = f\"{bot_name}: Aucun article trouvé.\\n\"\n",
    "    else:\n",
    "        sentence_qa = user_input\n",
    "        sentence = tokenize(user_input)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    response = f\"{bot_name}: {random.choice(intent['responses'])}\\n\"\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                response = f\"{bot_name}: {response}\\n\"\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                response = f\"{bot_name}: {answer}\\n\"\n",
    "\n",
    "    chat_history.insert(tk.END, response, \"bot\")\n",
    "    chat_history.config(state=tk.DISABLED)\n",
    "    chat_history.see(tk.END)\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot Interface\")\n",
    "\n",
    "frame = tk.Frame(root)\n",
    "frame.pack(pady=10)\n",
    "\n",
    "scrollbar = tk.Scrollbar(frame)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "chat_history = scrolledtext.ScrolledText(frame, width=50, height=20, wrap=tk.WORD, state=tk.DISABLED)\n",
    "chat_history.pack(padx=10, pady=10)\n",
    "\n",
    "input_field = Entry(root, font=(\"Helvetica\", 14))\n",
    "input_field.pack(fill=tk.X, padx=20, pady=10)\n",
    "\n",
    "input_field.bind(\"<Return>\", lambda _: send_message())\n",
    "\n",
    "send_button = tk.Button(root, text=\"Envoyer\", command=send_message)\n",
    "send_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#import json\n",
    "#import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Charger les données et le modèle pré-entraîné\n",
    "#with open('intents.json', 'r') as json_data:\n",
    "#   intents = json.load(json_data)\n",
    "\n",
    "#FILE = \"chatbot_model_base.pth\"\n",
    "#data = torch.load(FILE)\n",
    "\n",
    "#input_size = data[\"input_size\"]\n",
    "#hidden_size = data[\"hidden_size\"]\n",
    "#output_size = data[\"output_size\"]\n",
    "#all_words = data['all_words']\n",
    "#tags = data['tags']\n",
    "#model_state = data[\"model_state\"]\n",
    "\n",
    "# Charger le modèle\n",
    "#model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model.load_state_dict(model_state)\n",
    "#model.eval()\n",
    "\n",
    "# Nom du chatbot\n",
    "#bot_name = \"QueryBot\"\n",
    "\n",
    "#print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "#while True:\n",
    "    # Saisie utilisateur\n",
    "    #sentence = input(\"User: \")\n",
    "    #if sentence == 'quit':\n",
    "     #   break\n",
    "    \n",
    "    # Prédiction de l'intention\n",
    "   # sentence_qa = sentence\n",
    "   # sentence = tokenize(sentence)\n",
    "   # X = bag_of_words(sentence, all_words)\n",
    "    #X = X.reshape(1, X.shape[0])\n",
    "    #X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    #output = model(X)\n",
    "    #_, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    #tag = tags[predicted.item()]\n",
    "\n",
    "    #probs = torch.softmax(output, dim=1)\n",
    "    #prob = probs[0][predicted.item()]\n",
    "    \n",
    "    # Répondre en fonction de l'intention prédite ou en utilisant la question-réponse\n",
    "    #if prob.item() > 0.75:  # ajuster le seuil de probabilité selon la précision souhaitée\n",
    "     #   for intent in intents['intents']:\n",
    "      #      if tag == intent[\"tag\"]:\n",
    "       #         print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        #        break  # sortir après avoir trouvé une réponse appropriée\n",
    "   # else:\n",
    "    #    response = process_question(sentence_qa)\n",
    "     #   if response:\n",
    "      #      print(f\"{bot_name}: {response}\")\n",
    "       # else:\n",
    "        #    context = find_most_relevant_context(sentence_qa, dataset)\n",
    "         #   answer = generate_answer(sentence_qa, context)\n",
    "          #  print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Début de la conversation ('quit' pour finir la conversation...)\n",
    "#User: Quelle est la dernière nouvelle sur Tunisie Télécom ?\n",
    "#--------------------------------------------------\n",
    "#QueryBot: La dernière nouvelle sur Tunisie Télécom est : Tunisie Télécom lance un nouveau service de téléphonie. En savoir plus : http://example.com/article\n",
    "#==================================================\n",
    "#User: Combien y a-t-il eu d'articles positifs en 2020 ?\n",
    "#==================================================\n",
    "#QueryBot: Il y a 8 articles positifs en 2020.\n",
    "#--------------------------------------------------\n",
    "#User: Quelle année a eu le plus d'articles négatifs ?\n",
    "#==================================================\n",
    "#QueryBot: L'année avec le plus d'articles négatifs est 2022 avec 15 articles.\n",
    "#--------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d798cf",
   "metadata": {},
   "source": [
    "## Summary Model for the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "976cd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_orangesum = load_dataset(\"GEM/OrangeSum\", \"abstract\") # we can also specify \"title\" to obtain pairs of text-title\n",
    "#dataset_xlsum = load_dataset(\"csebuetnlp/xlsum\", \"french\")\n",
    "#dataset_mlsum = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eb973ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f8764d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    #if True:\n",
    "     #   text = text.split()\n",
    "      #  new_text = []\n",
    "       # for word in text:\n",
    "        #    if word in contractions:\n",
    "         #       new_text.append(contractions[word])\n",
    "          #  else:\n",
    "           #     new_text.append(word)\n",
    "        #text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"french\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97bcf5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (21401, 4), 'test': (1500, 4), 'validation': (1500, 4)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orangesum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fed47c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gem_id        0\n",
       "input         0\n",
       "target        0\n",
       "references    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_OS = pd.DataFrame(dataset_orangesum['test'])\n",
    "df_train_OS = pd.DataFrame(dataset_orangesum['train'])\n",
    "df_validation_OS = pd.DataFrame(dataset_orangesum['validation'])\n",
    "\n",
    "df_train_OS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2adc4597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gem_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OrangeSum_abstract-train-0</td>\n",
       "      <td>Thierry Mariani sur la liste du Rassemblement ...</td>\n",
       "      <td>L'information n'a pas été confirmée par l'inté...</td>\n",
       "      <td>[L'information n'a pas été confirmée par l'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OrangeSum_abstract-train-1</td>\n",
       "      <td>C'est désormais officiel : Alain Juppé n'est p...</td>\n",
       "      <td>Le maire de Bordeaux ne fait plus partie des R...</td>\n",
       "      <td>[Le maire de Bordeaux ne fait plus partie des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OrangeSum_abstract-train-2</td>\n",
       "      <td>La mesure est décriée par les avocats et les m...</td>\n",
       "      <td>En 2020, les tribunaux d'instance fusionnent a...</td>\n",
       "      <td>[En 2020, les tribunaux d'instance fusionnent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OrangeSum_abstract-train-3</td>\n",
       "      <td>Dans une interview accordée au Figaro mercredi...</td>\n",
       "      <td>Les médecins jugés \"gros prescripteurs d'arrêt...</td>\n",
       "      <td>[Les médecins jugés \"gros prescripteurs d'arrê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OrangeSum_abstract-train-4</td>\n",
       "      <td>Le préjudice est estimé à 2 millions d'euros. ...</td>\n",
       "      <td>Il aura fallu mobiliser 90 gendarmes pour cett...</td>\n",
       "      <td>[Il aura fallu mobiliser 90 gendarmes pour cet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       gem_id  \\\n",
       "0  OrangeSum_abstract-train-0   \n",
       "1  OrangeSum_abstract-train-1   \n",
       "2  OrangeSum_abstract-train-2   \n",
       "3  OrangeSum_abstract-train-3   \n",
       "4  OrangeSum_abstract-train-4   \n",
       "\n",
       "                                               input  \\\n",
       "0  Thierry Mariani sur la liste du Rassemblement ...   \n",
       "1  C'est désormais officiel : Alain Juppé n'est p...   \n",
       "2  La mesure est décriée par les avocats et les m...   \n",
       "3  Dans une interview accordée au Figaro mercredi...   \n",
       "4  Le préjudice est estimé à 2 millions d'euros. ...   \n",
       "\n",
       "                                              target  \\\n",
       "0  L'information n'a pas été confirmée par l'inté...   \n",
       "1  Le maire de Bordeaux ne fait plus partie des R...   \n",
       "2  En 2020, les tribunaux d'instance fusionnent a...   \n",
       "3  Les médecins jugés \"gros prescripteurs d'arrêt...   \n",
       "4  Il aura fallu mobiliser 90 gendarmes pour cett...   \n",
       "\n",
       "                                          references  \n",
       "0  [L'information n'a pas été confirmée par l'int...  \n",
       "1  [Le maire de Bordeaux ne fait plus partie des ...  \n",
       "2  [En 2020, les tribunaux d'instance fusionnent ...  \n",
       "3  [Les médecins jugés \"gros prescripteurs d'arrê...  \n",
       "4  [Il aura fallu mobiliser 90 gendarmes pour cet...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_OS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31703be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mega-\n",
      "[nltk_data]     Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "# Clean the summaries and texts\n",
    "clean_target = []\n",
    "for target in df_train_OS.target:\n",
    "    clean_target.append(clean_text(target, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_input = []\n",
    "for input in df_train_OS.input:\n",
    "    clean_input.append(clean_text(input))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "679217f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean News # 1\n",
      "l information n a pas été confirmée par l intéressé qui déclare toutefois étudier la question \n",
      "\n",
      "thierry mariani liste rassemblement national rn ex fn européennes affirme mardi 11 septembre chez pol nouvelle newsletter politique libération ancien député républicain ministre nicolas sarkozy point rejoindre troupes marine pen élections européennes 2019 ça va faire plus question calendrier obligé annoncer tout suite huit mois européennes ainsi assuré membre influent rn contacté franceinfo mariani a confirmé information élections juin sais numéro 1 liste a répondu ancien ministre transports reconnaît toutefois toujours cité franceinfo nom liste rn fait partie possibilités fréjus ville sympathique prévu rendre week end a ailleurs commenté twitter alors marine pen réunit cadres parti week end cité varoise proximité connue fnla proximité thierry mariani parti frontiste nouvelle sans alliés allons rester opposition longtemps temps renverser table front national a évolué regardons si accord rapprochement possibles déclaré interview donnée journal dimanche mars dernier puis avril bruissé rumeur rencontre entre ex député marine pen proposé figurer position éligible liste parti européennes conclusion hâtive époque écrit twitter mois thierry mariani cosigné tribune publiée valeurs actuelles côté élus frontistes appelant union droites\n",
      "\n",
      "Clean News # 2\n",
      "le maire de bordeaux ne fait plus partie des républicains et il tient à le montrer  lors de ses voeux à la presse  l édile a pris ses distances avec sa famille politique historique  qu il trouve trop proche de marine le pen \n",
      "\n",
      "désormais officiel alain juppé plus membre républicains ex premier ministre jacques chirac cofondateur ump 2002 paie plus cotisation auprès parti droite mercredi 9 janvier maire bordeaux a dénoncé glissement opère selon droite vers extême droite reconnais moins moins cette famille politique laquelle pourtant très attaché tristesse quittée a dérive vers thèses celles très proches extrême droite ambiguïté europe a déclaré face journalistes réunis assister voeux assiste cette espèce transfusion régulière thèmes fond a moments où demande entends radio membre lr rn a insisté maire bordeaux jour ex député thierry mariani annonçait départ lr rallier liste rassemblement national européennes mai prochain cela fait deux ans dit prenais distances républicains choses acquises depuis bien longtemps a tranché alain juppé ancien candidat primaire droite présidentielle 2017 a jamais fait mystère désaccord positions président républicains laurent wauquiez\n",
      "\n",
      "Clean News # 3\n",
      "en 2020  les tribunaux d instance fusionnent avec ceux de grande instance pour former un unique  tribunal judiciaire   c est la principale mesure de la réforme de la justice  portée par la garde des sceaux nicole belloubet \n",
      "\n",
      "mesure décriée avocats magistrats juridictions proximité excellence traitant litiges quotidien tribunaux instance apprêtent fusionner tribunaux grande instance cette réorganisation principales mesures réforme justice promulguée 23 mars professionnels inquiètent dévitalisation petites juridictions accès plus restreint juge réforme justice pourquoi avocats magistrats greffe colère dauphiné libérédepuis 1958 tribunaux instance ti tribunaux grande instance tgi partageaient contentieux civils selon répartition essentiellement fondée montant litige héritiers juges paix juges instance surnommés juges pauvres tranchaient toutes affaires lesquelles demande portait inférieures 10 000 euros expulsions locatives dettes impayées passant travaux mal exécutés conflits liés accidents circulation également compétents tutelles 1er janvier 285 tribunaux instance disparaissent ainsi 164 tgi france recours accru procédures dématérialisées quand tribunal instance situé commune tgi 57 ti concernés cette situation fusionnent former tribunal judiciaire quand ti situé commune différente comme ivry seine val marne condom gers molsheim bas rhin devient chambre détachée tribunal judiciaire appelé tribunal proximité alors particuliers pouvaient présenter directement greffe tribunal instance déposer requête réforme renforce recours accru procédures dématérialisées étend représentation obligatoire avocat exit aussi juge instance appellera désormais juge contentieux protection restera magistrat spécialisé affaires liées vulnérabilités économiques sociales garde sceaux dû renoncer supprimer cette fonction statutaire devant bronca opposants réforme compétences ajoutés décret quid leurs compétences deux principaux syndicats magistrats dénoncent flou autour question loi facilite création pôles spécialisés départements plusieurs tribunaux grande instance permet attribuer compétences supplémentaires tribunaux proximité mieux adapter besoins particuliers territoires souligne ministère justice ajouts compétences spécialisations décidés décrets après propositions chefs cours appel sans calendrier précis relèvent union syndicale magistrats usm syndicat magistrature sm plus gros bouleversement beaucoup cabinets juges instruction vont être supprimés juges application peines sait a aucune visibilité tacle présidente usm céline parisot calculs électoraux collègues savent chefs cour proposé ministre abonde katia dubreuil présidente sm déplorant absence concertation étonnée gouvernement souhaité différer annonces selon résultats électoraux république marche communes concernées comme écrivait canard enchaîné série articles fin octobre empêtrée cette polémique ministre bornée défendre toute partialité déplorant fusion conduite aveugle précipitation syndicats magistrats voient volonté faire économies échelle mutualisant effectifs greffes tribunaux aussi tribunaux conseils prud hommes malgré insistance garde sceaux répéter tous sites maintenus syndicats redoutent cette réforme prélude refonte carte judiciaire où tribunaux proximité vidés substance finiraient fermer depuis annonce cette fusion a près deux ans garde sceaux nicole belloubet invoque nécessité simplification lisibilité porte entrée unique justice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(3):\n",
    "    print(\"Clean News #\",i+1)\n",
    "    print(clean_target[i])\n",
    "    print(clean_input[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceec888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0606f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 111529\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_target)\n",
    "count_words(word_counts, clean_input)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18c1fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "from torchtext import vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfb29750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load of pretrained embeddings vectors :\n",
    "pretrained_vectors_fasttext = FastText(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5801c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained vocabulary contains 1152449 words, with embeddings in vectors of size 300\n"
     ]
    }
   ],
   "source": [
    "print(f'The pre-trained vocabulary contains {pretrained_vectors_fasttext.vectors.shape[0]} words, with embeddings in vectors of size {pretrained_vectors_fasttext.vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e373b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors_words = pretrained_vectors_fasttext.stoi.keys()\n",
    "pretrained_vectors_values = pretrained_vectors_fasttext.stoi.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af57ba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1663, -0.2434, -0.1298,  0.2558,  0.2620,  0.4340,  0.2389,  0.2846,\n",
       "        -0.0285,  0.2952, -0.1806, -0.0203,  0.2095,  0.2392,  0.4044,  0.2178,\n",
       "         0.3261,  0.1015,  0.1417, -0.1413, -0.1626, -0.6919, -0.1303,  0.5766,\n",
       "         0.2136, -0.0434, -0.4864,  0.2376, -0.3875,  0.0248,  0.5002,  0.4109,\n",
       "        -0.2349,  0.2109, -0.1231, -0.1220, -0.2864, -0.2508, -0.2469,  0.0470,\n",
       "         0.2941, -0.2932, -0.0470, -0.0928,  0.0722, -0.0158,  0.2090,  0.1393,\n",
       "         0.3059,  0.3177, -0.1812, -0.0239, -0.1266,  0.0802, -0.1903, -0.2608,\n",
       "        -0.3757, -0.0703,  0.3611,  0.2268, -0.1355,  0.2499, -0.0559, -0.1626,\n",
       "         0.1937,  0.3333, -0.0398, -0.0106, -0.2556, -0.2036,  0.3537,  0.0297,\n",
       "         0.0255, -0.1837,  0.1164, -0.3757,  0.2895, -0.2726,  0.0061, -0.2071,\n",
       "        -0.2901, -0.0297, -0.0647,  0.1851, -0.0209, -0.0855,  0.0574,  0.3292,\n",
       "        -0.3409, -0.4960, -0.1257, -0.3342,  0.0513, -0.0179,  0.0588,  0.0645,\n",
       "        -0.2976,  0.0638,  0.2410, -0.2550, -0.2986, -0.2085, -0.2570,  0.4697,\n",
       "         0.0947,  0.3796,  0.2746, -0.4441,  0.2341,  0.0768, -0.0301, -0.0469,\n",
       "         0.1435,  0.2824,  0.2580, -0.2594,  0.2596, -0.0618, -0.0595, -0.1947,\n",
       "         0.3589, -0.1353, -0.4342, -0.2739, -0.0385,  0.1818,  0.1305, -0.1849,\n",
       "         0.1211, -0.0591, -0.1312,  0.1912,  0.3404, -0.0382,  0.1397, -0.0051,\n",
       "        -0.2034,  0.0856, -0.0016,  0.0147,  0.0628,  0.1353, -0.2379,  0.1144,\n",
       "         0.3324, -0.2130, -0.1240,  0.2746,  0.5219,  0.4409, -0.2259,  0.0107,\n",
       "        -0.0836,  0.2050,  0.1743, -0.0184,  0.1690,  0.1539,  0.0077,  0.4725,\n",
       "        -0.3211, -0.0922, -0.1920, -0.0969, -0.3629,  0.2679,  0.2557, -0.0857,\n",
       "        -0.1151, -0.1501, -0.1330,  0.0605,  0.0886, -0.0343, -0.0945, -0.1892,\n",
       "         0.0479, -0.1171,  0.2026, -0.1340, -0.0248,  0.1926,  0.5280, -0.2757,\n",
       "         0.3417, -0.0139,  0.0388, -0.1377, -0.2876, -0.5761, -0.1273, -0.2940,\n",
       "         0.1769,  0.2692,  0.1415, -0.1625,  0.0234,  0.0058, -0.5362, -0.1814,\n",
       "         0.6860, -0.0921,  0.2931,  0.0845, -0.5805, -0.1804, -0.0793,  0.2759,\n",
       "        -0.1169,  0.6628,  0.1200,  0.2342, -0.0388, -0.1099,  0.0519,  0.1616,\n",
       "        -0.0277, -0.2439,  0.0415,  0.1379,  0.0974, -0.1609, -0.0360, -0.1602,\n",
       "         0.1139, -0.1470, -0.0983,  0.1412,  0.2697, -0.1490,  0.1723, -0.2127,\n",
       "         0.1073,  0.3893, -0.1609,  0.1174,  0.4289, -0.1584,  0.1251,  0.4409,\n",
       "        -0.1356,  0.0074, -0.5189, -0.0558,  0.0380, -0.1450, -0.0888, -0.2765,\n",
       "         0.3083,  0.1420, -0.0556,  0.3881, -0.0875,  0.0239, -0.1238, -0.0334,\n",
       "        -0.1900, -0.2796, -0.1100, -0.1270, -0.0018, -0.0813,  0.0889, -0.1201,\n",
       "        -0.0937, -0.0096, -0.1183, -0.1321, -0.0857,  0.0891,  0.2931, -0.0423,\n",
       "         0.2086,  0.5333, -0.2142, -0.2307, -0.3187, -0.0775, -0.1164, -0.2546,\n",
       "         0.0318, -0.0810,  0.1116,  0.1264, -0.1230, -0.2088,  0.0108,  0.4909,\n",
       "         0.1728, -0.1331, -0.0080, -0.3447,  0.5024, -0.0092, -0.0625, -0.1091,\n",
       "        -0.1239, -0.1336,  0.2848, -0.1123])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors_fasttext['maison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9b16e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 111529\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "\n",
    "#data_path = 'drive/MyDrive/numberbatch-fr.txt'\n",
    "#data_path = 'drive/MyDrive/Colab Notebooks/numberbatch-fr-clean.txt'\n",
    "#data_path = 'C:/Users/Giuseppe/Desktop/NLP/Embeddings/cc.fr.300.vec.gz'\n",
    "\n",
    "embeddings_index = {}#'rb' encoding='utf-8'\n",
    "\n",
    "#word_dict = []\n",
    "#with open(data_path, 'r', encoding='utf-8') as f: \n",
    " #   for line in f:\n",
    "        #values = line.split(' ')\n",
    "        #line = re.split(r'fr/',line)\n",
    "        #values = re.split(\" \", line[1])\n",
    "word = pretrained_vectors_fasttext.stoi.keys()#[12:]\n",
    "        #values = pretrained_vectors_fasttext.stoi.values()\n",
    "        #word_dict.append(word) \n",
    "for word in word_counts:\n",
    "    embedding = pretrained_vectors_fasttext[word]\n",
    "            #embedding = np.asarray(values[1:])#, dtype='float32'\n",
    "    embeddings_index[word] = embedding\n",
    "        #print(word)\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64c114b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from fastText: 0\n",
      "Percent of words that are missing from vocabulary: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from fastText:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54681902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 111529\n",
      "Number of words we will use: 111533\n",
      "Percent of words we will use: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in CN\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "744cf21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111533\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd5cdf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98609ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 5339165\n",
      "Total number of UNKs in headlines: 0\n",
      "Percent of words that are UNK: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_target and clean_input\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_target, word_count, unk_count = convert_to_ints(clean_target, word_count, unk_count)\n",
    "int_input, word_count, unk_count = convert_to_ints(clean_input, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2920323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0114d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean      34.393112\n",
      "std       12.316238\n",
      "min        3.000000\n",
      "25%       26.000000\n",
      "50%       34.000000\n",
      "75%       42.000000\n",
      "max      164.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean     216.088921\n",
      "std      106.650884\n",
      "min       11.000000\n",
      "25%      142.000000\n",
      "50%      192.000000\n",
      "75%      260.000000\n",
      "max     1884.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_target = create_lengths(int_target)\n",
    "lengths_input = create_lengths(int_input)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_target.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_input.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6821a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.0\n",
      "417.0\n",
      "525.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"input\"\n",
    "print(np.percentile(lengths_input.counts, 90))\n",
    "print(np.percentile(lengths_input.counts, 95))\n",
    "print(np.percentile(lengths_input.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af7ccc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n",
      "54.0\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"target\"\n",
    "print(np.percentile(lengths_target.counts, 90))\n",
    "print(np.percentile(lengths_target.counts, 95))\n",
    "print(np.percentile(lengths_target.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7cfae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4381c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19058\n",
      "19058\n"
     ]
    }
   ],
   "source": [
    "# takes a long time  , this is normal\n",
    "\n",
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove texts that include too many UNKs\n",
    "\n",
    "sorted_target = []\n",
    "sorted_input = []\n",
    "max_input_length = 377\n",
    "max_target_length = 70\n",
    "min_length = 2\n",
    "unk_input_limit = 1\n",
    "unk_target_limit = 0\n",
    "\n",
    "for length in range(min(lengths_input.counts), max_input_length): \n",
    "    for count, words in enumerate(int_target):\n",
    "        if (len(int_target[count]) >= min_length and\n",
    "            len(int_target[count]) <= max_target_length and\n",
    "            len(int_input[count]) >= min_length and\n",
    "            unk_counter(int_target[count]) <= unk_target_limit and\n",
    "            unk_counter(int_input[count]) <= unk_input_limit and\n",
    "            length == len(int_input[count])\n",
    "           ):\n",
    "            sorted_target.append(int_target[count])\n",
    "            sorted_input.append(int_input[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_target))\n",
    "print(len(sorted_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d918212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create placeholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f103bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85885f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "912cb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a38bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95b56c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
    "    #                                                                _zero_state_tensors(rnn_size, \n",
    "    #                                                                                    batch_size, \n",
    "    #                                                                                    tf.float32)) \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_decoder = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "        \n",
    "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_decoder = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "        \n",
    "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b8bb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35119ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02efdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58f8a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaa02f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~pencv-python-headless (C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python-headless (C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow== (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0)\n",
      "ERROR: No matching distribution found for tensorflow==\n",
      "WARNING: Ignoring invalid distribution ~pencv-python-headless (C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python-headless (C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python-headless (C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow== 1.13.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5adbcbd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set the graph to default to ensure that it is ready for training\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m train_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the model inputs    \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length \u001b[38;5;241m=\u001b[39m model_inputs()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Create the training and inference logits\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     training_logits, inference_logits \u001b[38;5;241m=\u001b[39m seq2seq_model(tf\u001b[38;5;241m.\u001b[39mreverse(input_data, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m     11\u001b[0m                                                       targets, \n\u001b[0;32m     12\u001b[0m                                                       keep_prob,   \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m                                                       vocab_to_int,\n\u001b[0;32m     20\u001b[0m                                                       batch_size)\n",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m, in \u001b[0;36mmodel_inputs\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_inputs\u001b[39m():\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Create placeholders for inputs to the model'''\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mint32, [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     targets \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mint32, [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     lr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5271949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data for training\n",
    "start = 10000\n",
    "end = start + 8000\n",
    "sorted_target_short = sorted_target[start:end]\n",
    "sorted_input_short = sorted_input[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_input_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_input_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57646ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.01#0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_input_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "  \n",
    "tf.reset_default_graph()\n",
    "checkpoint = \"C:/Users/Giuseppe/Desktop/NLP/best_model.ckpt\"  #300k sentence\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    #sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_target_short, sorted_input_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_input_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "                saver = tf.train.Saver() \n",
    "                saver.save(sess, checkpoint)\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "              \n",
    "                  \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n",
    "    saver.save(sess, checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
