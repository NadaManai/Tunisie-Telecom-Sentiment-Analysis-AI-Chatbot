{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a91d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d8ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, intents, all_words, tags):\n",
    "        self.intents = intents\n",
    "        self.all_words = all_words\n",
    "        self.tags = tags\n",
    "        self.xy = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        xy = []\n",
    "        for intent in self.intents:\n",
    "            tag = intent['tag']\n",
    "            for pattern in intent['patterns']:\n",
    "                tokenized_sentence = tokenize(pattern)\n",
    "                bag = bag_of_words(tokenized_sentence, self.all_words)\n",
    "                label = self.tags.index(tag)\n",
    "                xy.append((bag, label))\n",
    "        return xy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bag, label = self.xy[index]\n",
    "        return bag, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout avec taux de 0.5\n",
    "\n",
    "        # Initialisation des poids avec la méthode de He\n",
    "        torch.nn.init.kaiming_uniform_(self.l1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la première couche cachée\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la deuxième couche cachée\n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa7c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2500], Loss: 0.0446, Accuracy: 92.39837398373983\n",
      "Epoch [200/2500], Loss: 0.0552, Accuracy: 95.78861788617886\n",
      "Epoch [300/2500], Loss: 0.0001, Accuracy: 96.8970189701897\n",
      "Epoch [400/2500], Loss: 0.0001, Accuracy: 97.45934959349593\n",
      "Epoch [500/2500], Loss: 0.0261, Accuracy: 97.8048780487805\n",
      "Epoch [600/2500], Loss: 0.0005, Accuracy: 98.02981029810299\n",
      "Epoch [700/2500], Loss: 0.0003, Accuracy: 98.18931475029036\n",
      "Epoch [800/2500], Loss: 0.0001, Accuracy: 98.3170731707317\n",
      "Epoch [900/2500], Loss: 0.0706, Accuracy: 98.40018066847335\n",
      "Epoch [1000/2500], Loss: 0.0000, Accuracy: 98.46666666666667\n",
      "Epoch [1100/2500], Loss: 0.0000, Accuracy: 98.52845528455285\n",
      "Epoch [1200/2500], Loss: 0.0001, Accuracy: 98.58265582655827\n",
      "Epoch [1300/2500], Loss: 0.0000, Accuracy: 98.62976860537836\n",
      "Epoch [1400/2500], Loss: 0.0000, Accuracy: 98.6718931475029\n",
      "Epoch [1500/2500], Loss: 0.0000, Accuracy: 98.70189701897019\n",
      "Epoch [1600/2500], Loss: 0.0000, Accuracy: 98.72560975609755\n",
      "Epoch [1700/2500], Loss: 0.0000, Accuracy: 98.74653275944524\n",
      "Epoch [1800/2500], Loss: 0.0560, Accuracy: 98.7646793134598\n",
      "Epoch [1900/2500], Loss: 0.0000, Accuracy: 98.78733418913137\n",
      "Epoch [2000/2500], Loss: 0.0000, Accuracy: 98.80731707317074\n",
      "Epoch [2100/2500], Loss: 0.0000, Accuracy: 98.82346109175377\n",
      "Epoch [2200/2500], Loss: 0.0000, Accuracy: 98.84183296378419\n",
      "Epoch [2300/2500], Loss: 0.0000, Accuracy: 98.8557794273595\n",
      "Epoch [2400/2500], Loss: 0.0000, Accuracy: 98.87127371273712\n",
      "Epoch [2500/2500], Loss: 0.0000, Accuracy: 98.8829268292683\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_sentence = tokenize(pattern)\n",
    "        all_words.extend(tokenized_sentence)\n",
    "        xy.append((tokenized_sentence, tag))\n",
    "\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 512\n",
    "output_size = len(tags)\n",
    "\n",
    "dataset = ChatDataset(intents['intents'], all_words, tags)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "num_epochs = 2500\n",
    "correct=0\n",
    "total=0\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        accuracy =  100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a58e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to chatbot_model_base.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "    }\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76c397",
   "metadata": {},
   "source": [
    "## Deploying Q/A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c73379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ea958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset JSON\n",
    "with open('piaf-v1.1.json', 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Prétraitement des données\n",
    "paragraphs = []\n",
    "questions = []\n",
    "answers_start = []\n",
    "answers_end = []\n",
    "\n",
    "for article in dataset['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                answer_start = answer['answer_start']\n",
    "                answer_end = answer_start + len(answer['text'])\n",
    "\n",
    "                paragraphs.append(context)\n",
    "                questions.append(question)\n",
    "                answers_start.append(answer_start)\n",
    "                answers_end.append(answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74c291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle de question-réponse\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer une réponse à partir d'une question\n",
    "def generate_answer(question, context):\n",
    "    # Prétraitement des données\n",
    "    inputs = tokenizer_qa(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Prédiction de la réponse\n",
    "    outputs = model_qa(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Obtenir les indices de début et de fin de la réponse prédite\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    # Convertir les indices en texte\n",
    "    answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index+1]))\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f686d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour trouver le contexte le plus pertinent pour une question donnée\n",
    "def find_most_relevant_context(question, dataset):\n",
    "    # Utiliser TF-IDF pour représenter les contextes en vecteurs\n",
    "    contexts=[]\n",
    "    for article in dataset['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            contexts += [paragraph['context']]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(contexts)\n",
    "    \n",
    "    # Convertir la question en vecteur\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculer les similarités entre la question et les contextes\n",
    "    similarities = cosine_similarity(question_vector, vectors)\n",
    "    \n",
    "    # Trouver l'index du contexte le plus similaire\n",
    "    most_similar_index = similarities.argmax()\n",
    "    \n",
    "    # Renvoyer le contexte le plus similaire\n",
    "    return contexts[most_similar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05face",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8deac88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=146, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l3): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- IMPORT LIBRARY -----------------\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ------ IMPORT DATA & MODEL FOR BASE CASE ------\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdd2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(text):\n",
    "    cleaned_text = text.lower().strip() \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03284a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: donne moi un article positif de 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Voici un article positive pour l'année 2022: Tunisie Télécom et Huawei, ensemble pour la création de l’avenir\n",
      "Résumé:  Tunisie Telecom & Huawei Strategy to execution Summit 2022 s’est récemment tenue sous le thème 'Together, create the future' Tunisie Télécom and Huawei co-organise conjointement de développer un laboratoire d’innovation . Lin Xingshuo: 'Cette coopération avec Tunisie Télécom nous permet de mieux positionner la Tunisie dans ce nouveau monde numérique et de soutenir le pays dans son accélération digitale'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"`resume_download` is deprecated and will be removed in version 1.0.0\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Fonction pour résumer un article\n",
    "def summarize_article(content):\n",
    "    summarizer = pipeline(\"summarization\", min_length=30, max_length=90)\n",
    "    max_tokens = 1024  # Longueur maximale du modèle à ajuster si nécessaire\n",
    "    chunks = [content[i:i + max_tokens] for i in range(0, len(content), max_tokens)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Fonction pour récupérer les articles depuis MongoDB\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "\n",
    "# Chargement des intentions depuis le fichier JSON\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "# Chargement du modèle de chatbot\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Nom du bot\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Intent pour récupérer la dernière nouvelle sur Tunisie Télécom\n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "    # Intent pour gérer les questions sur les articles positifs/négatifs par année\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    # Intent pour récupérer un article positif ou négatif pour une certaine année\n",
    "    elif \"donne moi un article positif\" in sentence.lower() or \"donne moi un article négatif\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = 'positive' if \"positif\" in sentence.lower() else 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            filtered_articles = [article for article in articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "            \n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in filtered_articles if article['Publish Date'].year == year]\n",
    "            \n",
    "            if filtered_articles:\n",
    "                selected_article = random.choice(filtered_articles)\n",
    "                summary = summarize_article(selected_article['Content'])\n",
    "                print(f\"{bot_name}: Voici un article {sentiment_filter} pour l'année {selected_article['Publish Date'].year}: {selected_article['Title']}\\nRésumé: {summary}\")\n",
    "            else:\n",
    "                print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour cette année.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Intent pour résumer tous les articles positifs/négatifs d'une certaine année\n",
    "    elif \"résume moi tous les articles positifs\" in sentence.lower() or \"résume moi tous les articles négatifs\" in sentence.lower() or \"résume moi tous les articles de\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = None\n",
    "        if \"positifs\" in sentence.lower():\n",
    "            sentiment_filter = 'positive'\n",
    "        elif \"négatifs\" in sentence.lower():\n",
    "            sentiment_filter = 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in articles if article['Publish Date'].year == year]\n",
    "                if sentiment_filter:\n",
    "                    filtered_articles = [article for article in filtered_articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "                \n",
    "                if filtered_articles:\n",
    "                    for article in filtered_articles:\n",
    "                        summary = summarize_article(article['Content'])\n",
    "                        print(f\"{bot_name}: {article['Title']}\\nRésumé: {summary}\\n\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour l'année {year}.\")\n",
    "            else:\n",
    "                filtered_articles = [article for article in articles if sentiment_filter is None or article['SentimentCamembert'] == sentiment_filter]\n",
    "                if filtered_articles:\n",
    "                    for article in filtered_articles:\n",
    "                        summary = summarize_article(article['Content'])\n",
    "                        print(f\"{bot_name}: {article['Title']}\\nRésumé: {summary}\\n\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Si aucune intention spécifique n'est détectée, utiliser le modèle de classification de questions\n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        # Utiliser des fonctions appropriées pour tokenizer et transformer la phrase en vecteurs\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent['tag']:\n",
    "                    response = random.choice(intent['responses'])\n",
    "                    print(f\"{bot_name}: {response}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Je ne suis pas sûr de comprendre votre demande. Pouvez-vous reformuler?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4fef807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User:  donne moi un article positif de 2023\n",
      "QueryBot: Voici un article positive pour l'année 2023: Chez Tunisie Telecom, la sécurité est élevée au rang de priorité absolue !\n",
      "User:  Donne-moi un article négatif de 2022\n",
      "QueryBot: ...\n",
      "User:  donne-moi un article négatif de 2022\n",
      "QueryBot: ...\n",
      "User: donne-moi un article negatif de 2022\n",
      "QueryBot: ...\n",
      "User: donne-moi un article négatif de 2019\n",
      "QueryBot: ...\n",
      "User: donne moi un article positif de 2023\n",
      "QueryBot: Voici un article positive pour l'année 2023: Concours de la 6e: Inscription au service SMS à partir du 3 ...\n",
      "User: donne moi un article négatif de 2023\n",
      "QueryBot: Voici un article negative pour l'année 2023: Mécontentement croissant des abonnés de Tunisie Télécom face à la lenteur du débit internet\n",
      "User: donne moi un article positif de 2019\n",
      "QueryBot: Voici un article positive pour l'année 2019: Tunisie Telecom dévoile les gagnants de son jeu «CHAMPIONS QUIZ»\n",
      "User: donne moi un article positif de 2019\n",
      "QueryBot: Voici un article positive pour l'année 2019: Tunisie Telecom : Horaires de la double séance\n",
      "User: donne moi un article positif de 2019\n",
      "QueryBot: Voici un article positive pour l'année 2019: Tunisie Telecom : Horaires de la double séance\n",
      "User: donne moi un article positif de 2019\n",
      "QueryBot: Voici un article positive pour l'année 2019: Afif Chelbi: Cinq décennies de développement économique et social en Tunisie1961 - 2010\n",
      "User: donne moi un article positif de 2019\n",
      "QueryBot: Voici un article positive pour l'année 2019: Tunisie Telecom : Horaires de la double séance\n",
      "User: donne moi un article positif de 2019\n",
      "QueryBot: Voici un article positive pour l'année 2019: Tunisie Telecom dévoile les gagnants de son jeu «CHAMPIONS QUIZ»\n",
      "User: donne moi un article négatif de 2019\n",
      "QueryBot: Voici un article negative pour l'année 2019: Déception générale concernant la qualité du service clientèle de Tunisie Télécom\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDébut de la conversation (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pour finir la conversation...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "# Fonction pour récupérer les articles depuis MongoDB\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Chargement des intentions depuis le fichier JSON\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "# Chargement du modèle de chatbot\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Nom du bot\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Intent pour récupérer la dernière nouvelle sur Tunisie Télécom\n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "    # Intent pour gérer les questions sur les articles positifs/négatifs par année\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    # Intent pour récupérer un article positif ou négatif pour une certaine année\n",
    "    elif \"donne moi un article positif\" in sentence.lower() or \"donne moi un article négatif\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = 'positive' if \"positif\" in sentence.lower() else 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            filtered_articles = [article for article in articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "            \n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in filtered_articles if article['Publish Date'].year == year]\n",
    "            \n",
    "            if filtered_articles:\n",
    "                selected_article = random.choice(filtered_articles)\n",
    "                print(f\"{bot_name}: Voici un article {sentiment_filter} pour l'année {selected_article['Publish Date'].year}: {selected_article['Title']}\")\n",
    "            else:\n",
    "                print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour cette année.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Si aucune intention spécifique n'est détectée, utiliser le modèle de classification de questions\n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:  \n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                print(f\"{bot_name}: {response}\")\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                print(f\"{bot_name}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd7428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User:  donne moi un article positif de 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Voici un article positive pour l'année 2023: Bac: Les résultats de la session de contrôle par SMS\n",
      "Résumé:  L'inscription du baccalauréat 2023 (session de contrôle) a démarré, ce mardi à 10h du matin . Peuvent bénéficier de ce service les abonnés des opérateurs de Tunisie Télécom, Orange et Ooredoo .\n",
      "User: Résume-moi tous les articles négatifs de 2022\n",
      "QueryBot: résume-moi tous les articles négatifs de 2022\n",
      "User:  résume-moi tous les articles négatifs de 2022\n",
      "QueryBot: résume-moi tous les articles négatifs de 2022\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "\n",
    "# Fonction pour récupérer les articles depuis MongoDB\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Fonction pour résumer un article\n",
    "def summarize_article(content):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    max_tokens = 1024  # Longueur maximale du modèle à ajuster si nécessaire\n",
    "    chunks = [content[i:i + max_tokens] for i in range(0, len(content), max_tokens)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "# Chargement des intentions depuis le fichier JSON\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "# Chargement du modèle de chatbot\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Nom du bot\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Intent pour récupérer la dernière nouvelle sur Tunisie Télécom\n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "    # Intent pour gérer les questions sur les articles positifs/négatifs par année\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    # Intent pour récupérer un article positif ou négatif pour une certaine année\n",
    "    elif \"donne moi un article positif\" in sentence.lower() or \"donne moi un article négatif\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = 'positive' if \"positif\" in sentence.lower() else 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            filtered_articles = [article for article in articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "            \n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in filtered_articles if article['Publish Date'].year == year]\n",
    "            \n",
    "            if filtered_articles:\n",
    "                selected_article = random.choice(filtered_articles)\n",
    "                summary = summarize_article(selected_article['Content'])\n",
    "                print(f\"{bot_name}: Voici un article {sentiment_filter} pour l'année {selected_article['Publish Date'].year}: {selected_article['Title']}\\nRésumé: {summary}\")\n",
    "            else:\n",
    "                print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour cette année.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Intent pour résumer tous les articles positifs ou négatifs d'une certaine année\n",
    "    elif \"résume moi tous les articles positifs\" in sentence.lower() or \"résume moi tous les articles négatifs\" in sentence.lower() or \"résume moi tous les articles de\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = None\n",
    "        if \"positifs\" in sentence.lower():\n",
    "            sentiment_filter = 'positive'\n",
    "        elif \"négatifs\" in sentence.lower():\n",
    "            sentiment_filter = 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in articles if article['Publish Date'].year == year]\n",
    "                if sentiment_filter:\n",
    "                    filtered_articles = [article for article in filtered_articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "                \n",
    "                if filtered_articles:\n",
    "                    summaries = [summarize_article(article['Content']) for article in filtered_articles]\n",
    "                    for i, article in enumerate(filtered_articles):\n",
    "                        print(f\"{bot_name}: {article['Title']}\\nRésumé: {summaries[i]}\\n\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour l'année {year}.\")\n",
    "            else:\n",
    "                filtered_articles = [article for article in articles if sentiment_filter is None or article['SentimentCamembert'] == sentiment_filter]\n",
    "                if filtered_articles:\n",
    "                    summaries = [summarize_article(article['Content']) for article in filtered_articles]\n",
    "                    for i, article in enumerate(filtered_articles):\n",
    "                        print(f\"{bot_name}: {article['Title']}\\nRésumé: {summaries[i]}\\n\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Si aucune intention spécifique n'est détectée, utiliser le modèle de classification de questions\n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        # Utiliser des fonctions appropriées pour tokenizer et transformer la phrase en vecteurs\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:  \n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                print(f\"{bot_name}: {response}\")\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interface\n",
    "\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, Entry, END\n",
    "def send_message():\n",
    "    user_input = input_field.get()\n",
    "    input_field.delete(0, END)\n",
    "    chat_history.config(state=tk.NORMAL)\n",
    "    chat_history.insert(tk.END, f\"User: {user_input}\\n\", \"user\")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        root.quit()\n",
    "    elif \"tunisie télécom\" in user_input.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            response = f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\\n\"\n",
    "        else:\n",
    "            response = f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\\n\"\n",
    "    elif any(word in user_input.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', user_input)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article.get('SentimentCamembert', '')\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in user_input.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    response = f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\\n\"\n",
    "                else:\n",
    "                    response = f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\\n\"\n",
    "            elif \"combien d'articles négatifs\" in user_input.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    response = f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\\n\"\n",
    "                else:\n",
    "                    response = f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\\n\"\n",
    "            elif \"année avec le plus d'articles négatifs\" in user_input.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                response = f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\\n\"\n",
    "            elif \"année avec le plus d'articles positifs\" in user_input.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                response = f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\\n\"\n",
    "        else:\n",
    "            response = f\"{bot_name}: Aucun article trouvé.\\n\"\n",
    "    else:\n",
    "        sentence_qa = user_input\n",
    "        sentence = tokenize(user_input)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    response = f\"{bot_name}: {random.choice(intent['responses'])}\\n\"\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                response = f\"{bot_name}: {response}\\n\"\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                response = f\"{bot_name}: {answer}\\n\"\n",
    "\n",
    "    chat_history.insert(tk.END, response, \"bot\")\n",
    "    chat_history.config(state=tk.DISABLED)\n",
    "    chat_history.see(tk.END)\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot Interface\")\n",
    "\n",
    "frame = tk.Frame(root)\n",
    "frame.pack(pady=10)\n",
    "\n",
    "scrollbar = tk.Scrollbar(frame)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "chat_history = scrolledtext.ScrolledText(frame, width=50, height=20, wrap=tk.WORD, state=tk.DISABLED)\n",
    "chat_history.pack(padx=10, pady=10)\n",
    "\n",
    "input_field = Entry(root, font=(\"Helvetica\", 14))\n",
    "input_field.pack(fill=tk.X, padx=20, pady=10)\n",
    "\n",
    "input_field.bind(\"<Return>\", lambda _: send_message())\n",
    "\n",
    "send_button = tk.Button(root, text=\"Envoyer\", command=send_message)\n",
    "send_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f13e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dbe2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: résumé de tous les articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 150, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Résumé de l'article Tunisie Telecom et Topnet ensemble pour “couffins de Ramadan”:  Tunisie TelecometTopnetse mobilisent à nouveau pour la bonne cause en ce Ramadan 2024 . 75 couffins de Ramadan destinées aux populations les plus vulnérables dans une ambiance chaleureuse et de partage . Découvrez dans la vidéo de la préparation des Couffins de Ramadan par les équipes de TT et Topnet . Partager en famille les bénédictions de cette période .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Résumé de l'article Tunisie Telecom encourage les supporters et lance de nombreux jeux aux cadeaux conséquents:  Tunisie Telecomla marque généreuse, encourage son . équipe avec l’hymne ‘Ahom’, propose . supporters sur ses réseaux sociaux et via l‘application My TT toute une série de jeux .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Résumé de l'article Tunisie Telecom annonce le 1 Giga Rapido en fibre optique:  Tunisie Telecom annoncé le déploiement de la fibre optique 1 Giga Rapido . Tunisie telecom a  été le pionnier of the fibre-optique en Tunisie dès 1994 .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m     articles \u001b[38;5;241m=\u001b[39m fetch_articles()\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[1;32m--> 142\u001b[0m         summary \u001b[38;5;241m=\u001b[39m summarize_article(article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Assuming 'Content' field contains article content\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Résumé de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# If no specific intent is detected, use the question classification model\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[33], line 34\u001b[0m, in \u001b[0;36msummarize_article\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_article\u001b[39m(content):\n\u001b[1;32m---> 34\u001b[0m     summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m  \u001b[38;5;66;03m# Longueur maximale du modèle à ajuster si nécessaire\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m [content[i:i \u001b[38;5;241m+\u001b[39m max_tokens] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(content), max_tokens)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:904\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    902\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 904\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    905\u001b[0m             tokenizer_identifier, use_fast\u001b[38;5;241m=\u001b[39muse_fast, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[0;32m    906\u001b[0m         )\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:745\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1852\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   1855\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1856\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1857\u001b[0m     init_configuration,\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   1859\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1860\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1861\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1862\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   1863\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1865\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   2016\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2017\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2022\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bart\\tokenization_bart_fast.py:173\u001b[0m, in \u001b[0;36mBartTokenizerFast.__init__\u001b[1;34m(self, vocab_file, merges_file, tokenizer_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, trim_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    158\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    172\u001b[0m ):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    174\u001b[0m         vocab_file,\n\u001b[0;32m    175\u001b[0m         merges_file,\n\u001b[0;32m    176\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    177\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    178\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    179\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    180\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[0;32m    181\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[0;32m    182\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    183\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    184\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    185\u001b[0m         add_prefix_space\u001b[38;5;241m=\u001b[39madd_prefix_space,\n\u001b[0;32m    186\u001b[0m         trim_offsets\u001b[38;5;241m=\u001b[39mtrim_offsets,\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m     pre_tok_state \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer\u001b[38;5;241m.\u001b[39m__getstate__())\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_tok_state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_prefix_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_prefix_space) \u001b[38;5;241m!=\u001b[39m add_prefix_space:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1332\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1328\u001b[0m     )\n\u001b[0;32m   1330\u001b[0m converter_class \u001b[38;5;241m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[1;32m-> 1332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[38;5;241m.\u001b[39mconverted()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:361\u001b[0m, in \u001b[0;36mRobertaConverter.converted\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m vocab \u001b[38;5;241m=\u001b[39m ot\u001b[38;5;241m.\u001b[39mencoder\n\u001b[0;32m    359\u001b[0m merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ot\u001b[38;5;241m.\u001b[39mbpe_ranks\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m--> 361\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(\n\u001b[0;32m    362\u001b[0m     BPE(\n\u001b[0;32m    363\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m    364\u001b[0m         merges\u001b[38;5;241m=\u001b[39mmerges,\n\u001b[0;32m    365\u001b[0m         dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    366\u001b[0m         continuing_subword_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    367\u001b[0m         end_of_word_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    368\u001b[0m         fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    372\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39mot\u001b[38;5;241m.\u001b[39madd_prefix_space)\n\u001b[0;32m    373\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m decoders\u001b[38;5;241m.\u001b[39mByteLevel()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random  # Import ajouté\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning, append=True)\n",
    "\n",
    "\n",
    "# Function to fetch articles from MongoDB\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "        \n",
    "        # Optionally, fetch article content if needed for summarization\n",
    "        # article['Content'] = fetch_article_content(article['URL'])\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def summarize_article(content):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    max_tokens = 1024  # Longueur maximale du modèle à ajuster si nécessaire\n",
    "    chunks = [content[i:i + max_tokens] for i in range(0, len(content), max_tokens)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Loading intents from JSON file\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "# Loading chatbot model\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Bot's name\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Intent to fetch the latest news on Tunisie Télécom\n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "    # Intent to handle questions about positive/negative articles by year\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    # Intent to fetch a positive or negative article for a certain year\n",
    "    elif \"résumé d'un article positif\" in sentence.lower() or \"résumé d'un article négatif\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = 'positive' if \"positif\" in sentence.lower() else 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        matching_articles = []\n",
    "        for article in articles:\n",
    "            if article['SentimentCamembert'] == sentiment_filter:\n",
    "                if year_match and article['Publish Date'].year == int(year_match.group(0)):\n",
    "                    matching_articles.append(article)\n",
    "                elif not year_match:\n",
    "                    matching_articles.append(article)\n",
    "        \n",
    "        if matching_articles:\n",
    "            selected_article = random.choice(matching_articles)\n",
    "            summary = summarize_article(selected_article['Content'])  # Assuming 'Content' field contains article content\n",
    "            print(f\"{bot_name}: Résumé de l'article {selected_article['Title']} ({sentiment_filter}, année {selected_article['Publish Date'].year}): {summary}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour cette année.\")\n",
    "    \n",
    "    # Intent for summarizing all articles or specific sentiment articles from a certain year\n",
    "    elif \"résumé de tous les articles\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        for article in articles:\n",
    "            summary = summarize_article(article['Content'])  # Assuming 'Content' field contains article content\n",
    "            print(f\"{bot_name}: Résumé de l'article {article['Title']}: {summary}\")\n",
    "    \n",
    "    # If no specific intent is detected, use the question classification model\n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:  \n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                print(f\"{bot_name}: {response}\")\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#import json\n",
    "#import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Charger les données et le modèle pré-entraîné\n",
    "#with open('intents.json', 'r') as json_data:\n",
    "#   intents = json.load(json_data)\n",
    "\n",
    "#FILE = \"chatbot_model_base.pth\"\n",
    "#data = torch.load(FILE)\n",
    "\n",
    "#input_size = data[\"input_size\"]\n",
    "#hidden_size = data[\"hidden_size\"]\n",
    "#output_size = data[\"output_size\"]\n",
    "#all_words = data['all_words']\n",
    "#tags = data['tags']\n",
    "#model_state = data[\"model_state\"]\n",
    "\n",
    "# Charger le modèle\n",
    "#model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model.load_state_dict(model_state)\n",
    "#model.eval()\n",
    "\n",
    "# Nom du chatbot\n",
    "#bot_name = \"QueryBot\"\n",
    "\n",
    "#print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "#while True:\n",
    "    # Saisie utilisateur\n",
    "    #sentence = input(\"User: \")\n",
    "    #if sentence == 'quit':\n",
    "     #   break\n",
    "    \n",
    "    # Prédiction de l'intention\n",
    "   # sentence_qa = sentence\n",
    "   # sentence = tokenize(sentence)\n",
    "   # X = bag_of_words(sentence, all_words)\n",
    "    #X = X.reshape(1, X.shape[0])\n",
    "    #X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    #output = model(X)\n",
    "    #_, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    #tag = tags[predicted.item()]\n",
    "\n",
    "    #probs = torch.softmax(output, dim=1)\n",
    "    #prob = probs[0][predicted.item()]\n",
    "    \n",
    "    # Répondre en fonction de l'intention prédite ou en utilisant la question-réponse\n",
    "    #if prob.item() > 0.75:  # ajuster le seuil de probabilité selon la précision souhaitée\n",
    "     #   for intent in intents['intents']:\n",
    "      #      if tag == intent[\"tag\"]:\n",
    "       #         print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        #        break  # sortir après avoir trouvé une réponse appropriée\n",
    "   # else:\n",
    "    #    response = process_question(sentence_qa)\n",
    "     #   if response:\n",
    "      #      print(f\"{bot_name}: {response}\")\n",
    "       # else:\n",
    "        #    context = find_most_relevant_context(sentence_qa, dataset)\n",
    "         #   answer = generate_answer(sentence_qa, context)\n",
    "          #  print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User: donne moi un article positif de 2023\n",
    "#QueryBot: Voici un article positif pour l'année 2023: Title Article Positif 2023\n",
    "#User: combien d'articles positifs en 2022\n",
    "#QueryBot: Il y a 3 articles positifs en 2022.\n",
    "#User: année avec le plus d'articles négatifs\n",
    "#QueryBot: L'année avec le plus d'articles négatifs est 2021 avec 5 articles.\n",
    "#User: tunisie télécom\n",
    "#QueryBot: La dernière nouvelle sur Tunisie Télécom est : Title Dernière Nouvelle\n",
    "#User: donne moi un article négatif\n",
    "#QueryBot: Voici un article négatif pour l'année 2023: Title Article Négatif 2023\n",
    "#User: raconte-moi une blague\n",
    "#QueryBot: Désolé, je n'ai pas compris votre question. Pouvez-vous reformuler ?\n",
    "#User: quit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b336d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#import json\n",
    "#import torch\n",
    "#import pymongo\n",
    "#import re\n",
    "#from dateutil import parser\n",
    "#from datetime import datetime\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#def fetch_articles():\n",
    "    #client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    #db = client[\"StageTT\"]\n",
    "    #collection = db[\"TT\"]\n",
    "    #articles = list(collection.find({}))\n",
    "    \n",
    "  \n",
    "    #for article in articles:\n",
    "        #if isinstance(article['Publish Date'], str):\n",
    "            #try:\n",
    "               \n",
    "                #parsed_date = parser.parse(article['Publish Date'])\n",
    "               \n",
    "                #article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            #except ValueError:\n",
    "                #print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    #return articles\n",
    "\n",
    "\n",
    "#with open('intents.json', 'r') as json_data:\n",
    "    #intents = json.load(json_data)\n",
    "\n",
    "#FILE = \"chatbot_model_base.pth\"\n",
    "#data = torch.load(FILE)\n",
    "\n",
    "#input_size = data[\"input_size\"]\n",
    "#hidden_size = data[\"hidden_size\"]\n",
    "#output_size = data[\"output_size\"]\n",
    "#all_words = data['all_words']\n",
    "#tags = data['tags']\n",
    "#model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "#model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model.load_state_dict(model_state)\n",
    "#model.eval()\n",
    "\n",
    "\n",
    "#bot_name = \"QueryBot\"\n",
    "\n",
    "#print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "#while True:\n",
    "  \n",
    "    #sentence = input(\"User: \")\n",
    "    #if sentence == 'quit':\n",
    "        #break\n",
    "    \n",
    "  \n",
    "    #if \"tunisie télécom\" in sentence.lower():\n",
    "        #articles = fetch_articles()\n",
    "        #if articles:\n",
    "            #latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            #print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        #else:\n",
    "            #print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "\n",
    "    #elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        #year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        #articles = fetch_articles()\n",
    "        #if articles:\n",
    "            #articles_by_year = {}\n",
    "            #for article in articles:\n",
    "                #year = article['Publish Date'].year\n",
    "                #sentiment = article['SentimentCamembert']\n",
    "                #if year not in articles_by_year:\n",
    "                    #articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                #if sentiment == 'positive':\n",
    "                    #articles_by_year[year]['positive'] += 1\n",
    "                #elif sentiment == 'negative':\n",
    "                    #articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            #if \"combien d'articles positifs\" in sentence.lower():\n",
    "                #if year_match:\n",
    "                    #year = int(year_match.group(0))\n",
    "                    #positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    #print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                #else:\n",
    "                    #print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            #elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                #if year_match:\n",
    "                    #year = int(year_match.group(0))\n",
    "                    #negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    #print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                #else:\n",
    "                    #print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            #elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                #year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                #count = articles_by_year[year]['negative']\n",
    "                #print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            #elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                #year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                #count = articles_by_year[year]['positive']\n",
    "                #print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        #else:\n",
    "            #print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    \n",
    "    #else:\n",
    "        #sentence_qa = sentence\n",
    "        #sentence = tokenize(sentence)\n",
    "        #X = bag_of_words(sentence, all_words)\n",
    "        #X = X.reshape(1, X.shape[0])\n",
    "        #X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        #output = model(X)\n",
    "        #_, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        #tag = tags[predicted.item()]\n",
    "\n",
    "        #probs = torch.softmax(output, dim=1)\n",
    "        #prob = probs[0][predicted.item()]\n",
    "\n",
    "        \n",
    "        #if prob.item() > 0.75:  \n",
    "            #for intent in intents['intents']:\n",
    "                #if tag == intent[\"tag\"]:\n",
    "                    #print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    #break\n",
    "        #else:\n",
    "            #response = process_question(sentence_qa)\n",
    "            #if response:\n",
    "                #print(f\"{bot_name}: {response}\")\n",
    "            #else:\n",
    "                #context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                #answer = generate_answer(sentence_qa, context)\n",
    "                #print(f\"{bot_name}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dc6eb",
   "metadata": {},
   "source": [
    "## Summary Model for the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26605906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_orangesum = load_dataset(\"GEM/OrangeSum\", \"abstract\") # we can also specify \"title\" to obtain pairs of text-title\n",
    "#dataset_xlsum = load_dataset(\"csebuetnlp/xlsum\", \"french\")\n",
    "#dataset_mlsum = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2bd8d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e37c497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    #if True:\n",
    "     #   text = text.split()\n",
    "      #  new_text = []\n",
    "       # for word in text:\n",
    "        #    if word in contractions:\n",
    "         #       new_text.append(contractions[word])\n",
    "          #  else:\n",
    "           #     new_text.append(word)\n",
    "        #text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"french\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a76d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (21401, 4), 'test': (1500, 4), 'validation': (1500, 4)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orangesum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0e798af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gem_id        0\n",
       "input         0\n",
       "target        0\n",
       "references    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_OS = pd.DataFrame(dataset_orangesum['test'])\n",
    "df_train_OS = pd.DataFrame(dataset_orangesum['train'])\n",
    "df_validation_OS = pd.DataFrame(dataset_orangesum['validation'])\n",
    "\n",
    "df_train_OS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "121f7183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gem_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OrangeSum_abstract-train-0</td>\n",
       "      <td>Thierry Mariani sur la liste du Rassemblement ...</td>\n",
       "      <td>L'information n'a pas été confirmée par l'inté...</td>\n",
       "      <td>[L'information n'a pas été confirmée par l'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OrangeSum_abstract-train-1</td>\n",
       "      <td>C'est désormais officiel : Alain Juppé n'est p...</td>\n",
       "      <td>Le maire de Bordeaux ne fait plus partie des R...</td>\n",
       "      <td>[Le maire de Bordeaux ne fait plus partie des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OrangeSum_abstract-train-2</td>\n",
       "      <td>La mesure est décriée par les avocats et les m...</td>\n",
       "      <td>En 2020, les tribunaux d'instance fusionnent a...</td>\n",
       "      <td>[En 2020, les tribunaux d'instance fusionnent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OrangeSum_abstract-train-3</td>\n",
       "      <td>Dans une interview accordée au Figaro mercredi...</td>\n",
       "      <td>Les médecins jugés \"gros prescripteurs d'arrêt...</td>\n",
       "      <td>[Les médecins jugés \"gros prescripteurs d'arrê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OrangeSum_abstract-train-4</td>\n",
       "      <td>Le préjudice est estimé à 2 millions d'euros. ...</td>\n",
       "      <td>Il aura fallu mobiliser 90 gendarmes pour cett...</td>\n",
       "      <td>[Il aura fallu mobiliser 90 gendarmes pour cet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       gem_id  \\\n",
       "0  OrangeSum_abstract-train-0   \n",
       "1  OrangeSum_abstract-train-1   \n",
       "2  OrangeSum_abstract-train-2   \n",
       "3  OrangeSum_abstract-train-3   \n",
       "4  OrangeSum_abstract-train-4   \n",
       "\n",
       "                                               input  \\\n",
       "0  Thierry Mariani sur la liste du Rassemblement ...   \n",
       "1  C'est désormais officiel : Alain Juppé n'est p...   \n",
       "2  La mesure est décriée par les avocats et les m...   \n",
       "3  Dans une interview accordée au Figaro mercredi...   \n",
       "4  Le préjudice est estimé à 2 millions d'euros. ...   \n",
       "\n",
       "                                              target  \\\n",
       "0  L'information n'a pas été confirmée par l'inté...   \n",
       "1  Le maire de Bordeaux ne fait plus partie des R...   \n",
       "2  En 2020, les tribunaux d'instance fusionnent a...   \n",
       "3  Les médecins jugés \"gros prescripteurs d'arrêt...   \n",
       "4  Il aura fallu mobiliser 90 gendarmes pour cett...   \n",
       "\n",
       "                                          references  \n",
       "0  [L'information n'a pas été confirmée par l'int...  \n",
       "1  [Le maire de Bordeaux ne fait plus partie des ...  \n",
       "2  [En 2020, les tribunaux d'instance fusionnent ...  \n",
       "3  [Les médecins jugés \"gros prescripteurs d'arrê...  \n",
       "4  [Il aura fallu mobiliser 90 gendarmes pour cet...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_OS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0def09a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mega-\n",
      "[nltk_data]     Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "# Clean the summaries and texts\n",
    "clean_target = []\n",
    "for target in df_train_OS.target:\n",
    "    clean_target.append(clean_text(target, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_input = []\n",
    "for input in df_train_OS.input:\n",
    "    clean_input.append(clean_text(input))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f2cc04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean News # 1\n",
      "l information n a pas été confirmée par l intéressé qui déclare toutefois étudier la question \n",
      "\n",
      "thierry mariani liste rassemblement national rn ex fn européennes affirme mardi 11 septembre chez pol nouvelle newsletter politique libération ancien député républicain ministre nicolas sarkozy point rejoindre troupes marine pen élections européennes 2019 ça va faire plus question calendrier obligé annoncer tout suite huit mois européennes ainsi assuré membre influent rn contacté franceinfo mariani a confirmé information élections juin sais numéro 1 liste a répondu ancien ministre transports reconnaît toutefois toujours cité franceinfo nom liste rn fait partie possibilités fréjus ville sympathique prévu rendre week end a ailleurs commenté twitter alors marine pen réunit cadres parti week end cité varoise proximité connue fnla proximité thierry mariani parti frontiste nouvelle sans alliés allons rester opposition longtemps temps renverser table front national a évolué regardons si accord rapprochement possibles déclaré interview donnée journal dimanche mars dernier puis avril bruissé rumeur rencontre entre ex député marine pen proposé figurer position éligible liste parti européennes conclusion hâtive époque écrit twitter mois thierry mariani cosigné tribune publiée valeurs actuelles côté élus frontistes appelant union droites\n",
      "\n",
      "Clean News # 2\n",
      "le maire de bordeaux ne fait plus partie des républicains et il tient à le montrer  lors de ses voeux à la presse  l édile a pris ses distances avec sa famille politique historique  qu il trouve trop proche de marine le pen \n",
      "\n",
      "désormais officiel alain juppé plus membre républicains ex premier ministre jacques chirac cofondateur ump 2002 paie plus cotisation auprès parti droite mercredi 9 janvier maire bordeaux a dénoncé glissement opère selon droite vers extême droite reconnais moins moins cette famille politique laquelle pourtant très attaché tristesse quittée a dérive vers thèses celles très proches extrême droite ambiguïté europe a déclaré face journalistes réunis assister voeux assiste cette espèce transfusion régulière thèmes fond a moments où demande entends radio membre lr rn a insisté maire bordeaux jour ex député thierry mariani annonçait départ lr rallier liste rassemblement national européennes mai prochain cela fait deux ans dit prenais distances républicains choses acquises depuis bien longtemps a tranché alain juppé ancien candidat primaire droite présidentielle 2017 a jamais fait mystère désaccord positions président républicains laurent wauquiez\n",
      "\n",
      "Clean News # 3\n",
      "en 2020  les tribunaux d instance fusionnent avec ceux de grande instance pour former un unique  tribunal judiciaire   c est la principale mesure de la réforme de la justice  portée par la garde des sceaux nicole belloubet \n",
      "\n",
      "mesure décriée avocats magistrats juridictions proximité excellence traitant litiges quotidien tribunaux instance apprêtent fusionner tribunaux grande instance cette réorganisation principales mesures réforme justice promulguée 23 mars professionnels inquiètent dévitalisation petites juridictions accès plus restreint juge réforme justice pourquoi avocats magistrats greffe colère dauphiné libérédepuis 1958 tribunaux instance ti tribunaux grande instance tgi partageaient contentieux civils selon répartition essentiellement fondée montant litige héritiers juges paix juges instance surnommés juges pauvres tranchaient toutes affaires lesquelles demande portait inférieures 10 000 euros expulsions locatives dettes impayées passant travaux mal exécutés conflits liés accidents circulation également compétents tutelles 1er janvier 285 tribunaux instance disparaissent ainsi 164 tgi france recours accru procédures dématérialisées quand tribunal instance situé commune tgi 57 ti concernés cette situation fusionnent former tribunal judiciaire quand ti situé commune différente comme ivry seine val marne condom gers molsheim bas rhin devient chambre détachée tribunal judiciaire appelé tribunal proximité alors particuliers pouvaient présenter directement greffe tribunal instance déposer requête réforme renforce recours accru procédures dématérialisées étend représentation obligatoire avocat exit aussi juge instance appellera désormais juge contentieux protection restera magistrat spécialisé affaires liées vulnérabilités économiques sociales garde sceaux dû renoncer supprimer cette fonction statutaire devant bronca opposants réforme compétences ajoutés décret quid leurs compétences deux principaux syndicats magistrats dénoncent flou autour question loi facilite création pôles spécialisés départements plusieurs tribunaux grande instance permet attribuer compétences supplémentaires tribunaux proximité mieux adapter besoins particuliers territoires souligne ministère justice ajouts compétences spécialisations décidés décrets après propositions chefs cours appel sans calendrier précis relèvent union syndicale magistrats usm syndicat magistrature sm plus gros bouleversement beaucoup cabinets juges instruction vont être supprimés juges application peines sait a aucune visibilité tacle présidente usm céline parisot calculs électoraux collègues savent chefs cour proposé ministre abonde katia dubreuil présidente sm déplorant absence concertation étonnée gouvernement souhaité différer annonces selon résultats électoraux république marche communes concernées comme écrivait canard enchaîné série articles fin octobre empêtrée cette polémique ministre bornée défendre toute partialité déplorant fusion conduite aveugle précipitation syndicats magistrats voient volonté faire économies échelle mutualisant effectifs greffes tribunaux aussi tribunaux conseils prud hommes malgré insistance garde sceaux répéter tous sites maintenus syndicats redoutent cette réforme prélude refonte carte judiciaire où tribunaux proximité vidés substance finiraient fermer depuis annonce cette fusion a près deux ans garde sceaux nicole belloubet invoque nécessité simplification lisibilité porte entrée unique justice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(3):\n",
    "    print(\"Clean News #\",i+1)\n",
    "    print(clean_target[i])\n",
    "    print(clean_input[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7920f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d15cbb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 111529\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_target)\n",
    "count_words(word_counts, clean_input)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "354ece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "from torchtext.vocab import Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "235736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load of pretrained embeddings vectors :\n",
    "pretrained_vectors_fasttext = FastText(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a073f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained vocabulary contains 1152449 words, with embeddings in vectors of size 300\n"
     ]
    }
   ],
   "source": [
    "print(f'The pre-trained vocabulary contains {pretrained_vectors_fasttext.vectors.shape[0]} words, with embeddings in vectors of size {pretrained_vectors_fasttext.vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22f707b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors_words = pretrained_vectors_fasttext.stoi.keys()\n",
    "pretrained_vectors_values = pretrained_vectors_fasttext.stoi.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "311cebf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1663, -0.2434, -0.1298,  0.2558,  0.2620,  0.4340,  0.2389,  0.2846,\n",
       "        -0.0285,  0.2952, -0.1806, -0.0203,  0.2095,  0.2392,  0.4044,  0.2178,\n",
       "         0.3261,  0.1015,  0.1417, -0.1413, -0.1626, -0.6919, -0.1303,  0.5766,\n",
       "         0.2136, -0.0434, -0.4864,  0.2376, -0.3875,  0.0248,  0.5002,  0.4109,\n",
       "        -0.2349,  0.2109, -0.1231, -0.1220, -0.2864, -0.2508, -0.2469,  0.0470,\n",
       "         0.2941, -0.2932, -0.0470, -0.0928,  0.0722, -0.0158,  0.2090,  0.1393,\n",
       "         0.3059,  0.3177, -0.1812, -0.0239, -0.1266,  0.0802, -0.1903, -0.2608,\n",
       "        -0.3757, -0.0703,  0.3611,  0.2268, -0.1355,  0.2499, -0.0559, -0.1626,\n",
       "         0.1937,  0.3333, -0.0398, -0.0106, -0.2556, -0.2036,  0.3537,  0.0297,\n",
       "         0.0255, -0.1837,  0.1164, -0.3757,  0.2895, -0.2726,  0.0061, -0.2071,\n",
       "        -0.2901, -0.0297, -0.0647,  0.1851, -0.0209, -0.0855,  0.0574,  0.3292,\n",
       "        -0.3409, -0.4960, -0.1257, -0.3342,  0.0513, -0.0179,  0.0588,  0.0645,\n",
       "        -0.2976,  0.0638,  0.2410, -0.2550, -0.2986, -0.2085, -0.2570,  0.4697,\n",
       "         0.0947,  0.3796,  0.2746, -0.4441,  0.2341,  0.0768, -0.0301, -0.0469,\n",
       "         0.1435,  0.2824,  0.2580, -0.2594,  0.2596, -0.0618, -0.0595, -0.1947,\n",
       "         0.3589, -0.1353, -0.4342, -0.2739, -0.0385,  0.1818,  0.1305, -0.1849,\n",
       "         0.1211, -0.0591, -0.1312,  0.1912,  0.3404, -0.0382,  0.1397, -0.0051,\n",
       "        -0.2034,  0.0856, -0.0016,  0.0147,  0.0628,  0.1353, -0.2379,  0.1144,\n",
       "         0.3324, -0.2130, -0.1240,  0.2746,  0.5219,  0.4409, -0.2259,  0.0107,\n",
       "        -0.0836,  0.2050,  0.1743, -0.0184,  0.1690,  0.1539,  0.0077,  0.4725,\n",
       "        -0.3211, -0.0922, -0.1920, -0.0969, -0.3629,  0.2679,  0.2557, -0.0857,\n",
       "        -0.1151, -0.1501, -0.1330,  0.0605,  0.0886, -0.0343, -0.0945, -0.1892,\n",
       "         0.0479, -0.1171,  0.2026, -0.1340, -0.0248,  0.1926,  0.5280, -0.2757,\n",
       "         0.3417, -0.0139,  0.0388, -0.1377, -0.2876, -0.5761, -0.1273, -0.2940,\n",
       "         0.1769,  0.2692,  0.1415, -0.1625,  0.0234,  0.0058, -0.5362, -0.1814,\n",
       "         0.6860, -0.0921,  0.2931,  0.0845, -0.5805, -0.1804, -0.0793,  0.2759,\n",
       "        -0.1169,  0.6628,  0.1200,  0.2342, -0.0388, -0.1099,  0.0519,  0.1616,\n",
       "        -0.0277, -0.2439,  0.0415,  0.1379,  0.0974, -0.1609, -0.0360, -0.1602,\n",
       "         0.1139, -0.1470, -0.0983,  0.1412,  0.2697, -0.1490,  0.1723, -0.2127,\n",
       "         0.1073,  0.3893, -0.1609,  0.1174,  0.4289, -0.1584,  0.1251,  0.4409,\n",
       "        -0.1356,  0.0074, -0.5189, -0.0558,  0.0380, -0.1450, -0.0888, -0.2765,\n",
       "         0.3083,  0.1420, -0.0556,  0.3881, -0.0875,  0.0239, -0.1238, -0.0334,\n",
       "        -0.1900, -0.2796, -0.1100, -0.1270, -0.0018, -0.0813,  0.0889, -0.1201,\n",
       "        -0.0937, -0.0096, -0.1183, -0.1321, -0.0857,  0.0891,  0.2931, -0.0423,\n",
       "         0.2086,  0.5333, -0.2142, -0.2307, -0.3187, -0.0775, -0.1164, -0.2546,\n",
       "         0.0318, -0.0810,  0.1116,  0.1264, -0.1230, -0.2088,  0.0108,  0.4909,\n",
       "         0.1728, -0.1331, -0.0080, -0.3447,  0.5024, -0.0092, -0.0625, -0.1091,\n",
       "        -0.1239, -0.1336,  0.2848, -0.1123])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors_fasttext['maison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c995fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 111529\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "\n",
    "#data_path = 'drive/MyDrive/numberbatch-fr.txt'\n",
    "#data_path = 'drive/MyDrive/Colab Notebooks/numberbatch-fr-clean.txt'\n",
    "#data_path = 'C:/Users/Giuseppe/Desktop/NLP/Embeddings/cc.fr.300.vec.gz'\n",
    "\n",
    "embeddings_index = {}#'rb' encoding='utf-8'\n",
    "\n",
    "#word_dict = []\n",
    "#with open(data_path, 'r', encoding='utf-8') as f: \n",
    " #   for line in f:\n",
    "        #values = line.split(' ')\n",
    "        #line = re.split(r'fr/',line)\n",
    "        #values = re.split(\" \", line[1])\n",
    "word = pretrained_vectors_fasttext.stoi.keys()#[12:]\n",
    "        #values = pretrained_vectors_fasttext.stoi.values()\n",
    "        #word_dict.append(word) \n",
    "for word in word_counts:\n",
    "    embedding = pretrained_vectors_fasttext[word]\n",
    "            #embedding = np.asarray(values[1:])#, dtype='float32'\n",
    "    embeddings_index[word] = embedding\n",
    "        #print(word)\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3fd1780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from fastText: 0\n",
      "Percent of words that are missing from vocabulary: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from fastText:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a09862a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 111529\n",
      "Number of words we will use: 111533\n",
      "Percent of words we will use: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in CN\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe9464a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111533\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "070ec8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5b725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 5339165\n",
      "Total number of UNKs in headlines: 0\n",
      "Percent of words that are UNK: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_target and clean_input\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_target, word_count, unk_count = convert_to_ints(clean_target, word_count, unk_count)\n",
    "int_input, word_count, unk_count = convert_to_ints(clean_input, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4d7ba733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba3497ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean      34.393112\n",
      "std       12.316238\n",
      "min        3.000000\n",
      "25%       26.000000\n",
      "50%       34.000000\n",
      "75%       42.000000\n",
      "max      164.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean     216.088921\n",
      "std      106.650884\n",
      "min       11.000000\n",
      "25%      142.000000\n",
      "50%      192.000000\n",
      "75%      260.000000\n",
      "max     1884.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_target = create_lengths(int_target)\n",
    "lengths_input = create_lengths(int_input)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_target.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_input.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f3bde881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.0\n",
      "417.0\n",
      "525.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"input\"\n",
    "print(np.percentile(lengths_input.counts, 90))\n",
    "print(np.percentile(lengths_input.counts, 95))\n",
    "print(np.percentile(lengths_input.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c28e724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n",
      "54.0\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"target\"\n",
    "print(np.percentile(lengths_target.counts, 90))\n",
    "print(np.percentile(lengths_target.counts, 95))\n",
    "print(np.percentile(lengths_target.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9519aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1ea53d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19058\n",
      "19058\n"
     ]
    }
   ],
   "source": [
    "sorted_target = []\n",
    "sorted_input = []\n",
    "max_input_length = 377\n",
    "max_target_length = 70\n",
    "min_length = 2\n",
    "unk_input_limit = 1\n",
    "unk_target_limit = 0\n",
    "\n",
    "for length in range(min(lengths_input.counts), max_input_length): \n",
    "    for count, words in enumerate(int_target):\n",
    "        if (len(int_target[count]) >= min_length and\n",
    "            len(int_target[count]) <= max_target_length and\n",
    "            len(int_input[count]) >= min_length and\n",
    "            unk_counter(int_target[count]) <= unk_target_limit and\n",
    "            unk_counter(int_input[count]) <= unk_input_limit and\n",
    "            length == len(int_input[count])\n",
    "           ):\n",
    "            sorted_target.append(int_target[count])\n",
    "            sorted_input.append(int_input[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_target))\n",
    "print(len(sorted_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddcfd9",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
