{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a91d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d8ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, intents, all_words, tags):\n",
    "        self.intents = intents\n",
    "        self.all_words = all_words\n",
    "        self.tags = tags\n",
    "        self.xy = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        xy = []\n",
    "        for intent in self.intents:\n",
    "            tag = intent['tag']\n",
    "            for pattern in intent['patterns']:\n",
    "                tokenized_sentence = tokenize(pattern)\n",
    "                bag = bag_of_words(tokenized_sentence, self.all_words)\n",
    "                label = self.tags.index(tag)\n",
    "                xy.append((bag, label))\n",
    "        return xy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bag, label = self.xy[index]\n",
    "        return bag, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout avec taux de 0.5\n",
    "\n",
    "        # Initialisation des poids avec la méthode de He\n",
    "        torch.nn.init.kaiming_uniform_(self.l1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la première couche cachée\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la deuxième couche cachée\n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa7c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2500], Loss: 0.0073, Accuracy: 92.46341463414635\n",
      "Epoch [200/2500], Loss: 0.0031, Accuracy: 95.79674796747967\n",
      "Epoch [300/2500], Loss: 0.0004, Accuracy: 96.92140921409214\n",
      "Epoch [400/2500], Loss: 0.0001, Accuracy: 97.44918699186992\n",
      "Epoch [500/2500], Loss: 0.0713, Accuracy: 97.80813008130082\n",
      "Epoch [600/2500], Loss: 0.0020, Accuracy: 98.02439024390245\n",
      "Epoch [700/2500], Loss: 0.0000, Accuracy: 98.18350754936121\n",
      "Epoch [800/2500], Loss: 0.0000, Accuracy: 98.3180894308943\n",
      "Epoch [900/2500], Loss: 0.0000, Accuracy: 98.41824751580849\n",
      "Epoch [1000/2500], Loss: 0.0000, Accuracy: 98.49512195121952\n",
      "Epoch [1100/2500], Loss: 0.0000, Accuracy: 98.54988913525499\n",
      "Epoch [1200/2500], Loss: 0.0000, Accuracy: 98.6050135501355\n",
      "Epoch [1300/2500], Loss: 0.0000, Accuracy: 98.64602876797998\n",
      "Epoch [1400/2500], Loss: 0.0000, Accuracy: 98.68524970963995\n",
      "Epoch [1500/2500], Loss: 0.0395, Accuracy: 98.71978319783197\n",
      "Epoch [1600/2500], Loss: 0.0000, Accuracy: 98.744918699187\n",
      "Epoch [1700/2500], Loss: 0.0000, Accuracy: 98.76948828311812\n",
      "Epoch [1800/2500], Loss: 0.0007, Accuracy: 98.79177958446252\n",
      "Epoch [1900/2500], Loss: 0.0000, Accuracy: 98.80958493795464\n",
      "Epoch [2000/2500], Loss: 0.0000, Accuracy: 98.82682926829268\n",
      "Epoch [2100/2500], Loss: 0.0000, Accuracy: 98.84010840108401\n",
      "Epoch [2200/2500], Loss: 0.0000, Accuracy: 98.85328898743533\n",
      "Epoch [2300/2500], Loss: 0.0000, Accuracy: 98.86284906327325\n",
      "Epoch [2400/2500], Loss: 0.0000, Accuracy: 98.87432249322494\n",
      "Epoch [2500/2500], Loss: 0.0000, Accuracy: 98.88162601626016\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_sentence = tokenize(pattern)\n",
    "        all_words.extend(tokenized_sentence)\n",
    "        xy.append((tokenized_sentence, tag))\n",
    "\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 512\n",
    "output_size = len(tags)\n",
    "\n",
    "dataset = ChatDataset(intents['intents'], all_words, tags)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "num_epochs = 2500\n",
    "correct=0\n",
    "total=0\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        accuracy =  100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a58e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to chatbot_model_base.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "    }\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76c397",
   "metadata": {},
   "source": [
    "## Deploying Q/A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c73379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ea958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset JSON\n",
    "with open('piaf-v1.1.json', 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Prétraitement des données\n",
    "paragraphs = []\n",
    "questions = []\n",
    "answers_start = []\n",
    "answers_end = []\n",
    "\n",
    "for article in dataset['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                answer_start = answer['answer_start']\n",
    "                answer_end = answer_start + len(answer['text'])\n",
    "\n",
    "                paragraphs.append(context)\n",
    "                questions.append(question)\n",
    "                answers_start.append(answer_start)\n",
    "                answers_end.append(answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74c291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle de question-réponse\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer une réponse à partir d'une question\n",
    "def generate_answer(question, context):\n",
    "    # Prétraitement des données\n",
    "    inputs = tokenizer_qa(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Prédiction de la réponse\n",
    "    outputs = model_qa(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Obtenir les indices de début et de fin de la réponse prédite\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    # Convertir les indices en texte\n",
    "    answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index+1]))\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f686d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour trouver le contexte le plus pertinent pour une question donnée\n",
    "def find_most_relevant_context(question, dataset):\n",
    "    # Utiliser TF-IDF pour représenter les contextes en vecteurs\n",
    "    contexts=[]\n",
    "    for article in dataset['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            contexts += [paragraph['context']]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(contexts)\n",
    "    \n",
    "    # Convertir la question en vecteur\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculer les similarités entre la question et les contextes\n",
    "    similarities = cosine_similarity(question_vector, vectors)\n",
    "    \n",
    "    # Trouver l'index du contexte le plus similaire\n",
    "    most_similar_index = similarities.argmax()\n",
    "    \n",
    "    # Renvoyer le contexte le plus similaire\n",
    "    return contexts[most_similar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05face",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8deac88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=146, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l3): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- IMPORT LIBRARY -----------------\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ------ IMPORT DATA & MODEL FOR BASE CASE ------\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdd2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(text):\n",
    "    cleaned_text = text.lower().strip() \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6270cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: blague\n",
      "QueryBot: J'ai mange une horloge hier, c'etait tres chronophage\n",
      "User: devinette\n",
      "QueryBot: Pourquoi un velo ne peut-il pas tenir tout seul ?.....Il est trop fatigue.\n",
      "User: bonjour\n",
      "QueryBot: Content de vous revoir\n",
      "User: ça va?\n",
      "QueryBot: Tout va bien.. Et vous?\n",
      "User: bien\n",
      "QueryBot: Content de le savoir!\n",
      "User:  Quelle est la dernière nouvelle sur Tunisie Télécom ?\n",
      "QueryBot: La dernière nouvelle sur Tunisie Télécom est : Tunisie Télécom et l’AtGmo : “Run of Heroes” pour soutenir les malades d’hémopathies\n",
      "User: Combien d'articles positifs y a-t-il en 2023 ?\n",
      "QueryBot: Il y a 9 articles positifs en 2023.\n",
      "User: Quelle est l'année avec le plus d'articles négatifs ?\n",
      "QueryBot: L'année avec le plus d'articles négatifs est 2019 avec 7 articles.\n",
      "User: Combien d'articles négatifs y a-t-il en 2024 ?\n",
      "QueryBot: Il y a 0 articles négatifs en 2024.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDébut de la conversation (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pour finir la conversation...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Saisie utilisateur\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "  \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "               \n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "               \n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "  \n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "  \n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        \n",
    "        if prob.item() > 0.75:  \n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    break\n",
    "        else:\n",
    "            response = process_question(sentence_qa)\n",
    "            if response:\n",
    "                print(f\"{bot_name}: {response}\")\n",
    "            else:\n",
    "                context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                answer = generate_answer(sentence_qa, context)\n",
    "                print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#import json\n",
    "#import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Charger les données et le modèle pré-entraîné\n",
    "#with open('intents.json', 'r') as json_data:\n",
    "#   intents = json.load(json_data)\n",
    "\n",
    "#FILE = \"chatbot_model_base.pth\"\n",
    "#data = torch.load(FILE)\n",
    "\n",
    "#input_size = data[\"input_size\"]\n",
    "#hidden_size = data[\"hidden_size\"]\n",
    "#output_size = data[\"output_size\"]\n",
    "#all_words = data['all_words']\n",
    "#tags = data['tags']\n",
    "#model_state = data[\"model_state\"]\n",
    "\n",
    "# Charger le modèle\n",
    "#model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model.load_state_dict(model_state)\n",
    "#model.eval()\n",
    "\n",
    "# Nom du chatbot\n",
    "#bot_name = \"QueryBot\"\n",
    "\n",
    "#print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "#while True:\n",
    "    # Saisie utilisateur\n",
    "    #sentence = input(\"User: \")\n",
    "    #if sentence == 'quit':\n",
    "     #   break\n",
    "    \n",
    "    # Prédiction de l'intention\n",
    "   # sentence_qa = sentence\n",
    "   # sentence = tokenize(sentence)\n",
    "   # X = bag_of_words(sentence, all_words)\n",
    "    #X = X.reshape(1, X.shape[0])\n",
    "    #X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    #output = model(X)\n",
    "    #_, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    #tag = tags[predicted.item()]\n",
    "\n",
    "    #probs = torch.softmax(output, dim=1)\n",
    "    #prob = probs[0][predicted.item()]\n",
    "    \n",
    "    # Répondre en fonction de l'intention prédite ou en utilisant la question-réponse\n",
    "    #if prob.item() > 0.75:  # ajuster le seuil de probabilité selon la précision souhaitée\n",
    "     #   for intent in intents['intents']:\n",
    "      #      if tag == intent[\"tag\"]:\n",
    "       #         print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        #        break  # sortir après avoir trouvé une réponse appropriée\n",
    "   # else:\n",
    "    #    response = process_question(sentence_qa)\n",
    "     #   if response:\n",
    "      #      print(f\"{bot_name}: {response}\")\n",
    "       # else:\n",
    "        #    context = find_most_relevant_context(sentence_qa, dataset)\n",
    "         #   answer = generate_answer(sentence_qa, context)\n",
    "          #  print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Début de la conversation ('quit' pour finir la conversation...)\n",
    "#User: Quelle est la dernière nouvelle sur Tunisie Télécom ?\n",
    "#--------------------------------------------------\n",
    "#QueryBot: La dernière nouvelle sur Tunisie Télécom est : Tunisie Télécom lance un nouveau service de téléphonie. En savoir plus : http://example.com/article\n",
    "#==================================================\n",
    "#User: Combien y a-t-il eu d'articles positifs en 2020 ?\n",
    "#==================================================\n",
    "#QueryBot: Il y a 8 articles positifs en 2020.\n",
    "#--------------------------------------------------\n",
    "#User: Quelle année a eu le plus d'articles négatifs ?\n",
    "#==================================================\n",
    "#QueryBot: L'année avec le plus d'articles négatifs est 2022 avec 15 articles.\n",
    "#--------------------------------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
