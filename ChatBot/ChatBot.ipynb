{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f7a1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3f103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91b066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a91d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3d8ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, intents, all_words, tags):\n",
    "        self.intents = intents\n",
    "        self.all_words = all_words\n",
    "        self.tags = tags\n",
    "        self.xy = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        xy = []\n",
    "        for intent in self.intents:\n",
    "            tag = intent['tag']\n",
    "            for pattern in intent['patterns']:\n",
    "                tokenized_sentence = tokenize(pattern)\n",
    "                bag = bag_of_words(tokenized_sentence, self.all_words)\n",
    "                label = self.tags.index(tag)\n",
    "                xy.append((bag, label))\n",
    "        return xy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bag, label = self.xy[index]\n",
    "        return bag, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout avec taux de 0.5\n",
    "\n",
    "        # Initialisation des poids avec la méthode de He\n",
    "        torch.nn.init.kaiming_uniform_(self.l1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la première couche cachée\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la deuxième couche cachée\n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aa7c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2500], Loss: 0.0149, Accuracy: 92.66666666666667\n",
      "Epoch [200/2500], Loss: 0.0004, Accuracy: 95.91056910569105\n",
      "Epoch [300/2500], Loss: 0.0004, Accuracy: 97.00542005420054\n",
      "Epoch [400/2500], Loss: 0.0003, Accuracy: 97.53658536585365\n",
      "Epoch [500/2500], Loss: 0.0001, Accuracy: 97.84065040650407\n",
      "Epoch [600/2500], Loss: 0.0000, Accuracy: 98.05420054200542\n",
      "Epoch [700/2500], Loss: 0.0000, Accuracy: 98.20557491289199\n",
      "Epoch [800/2500], Loss: 0.0000, Accuracy: 98.33231707317073\n",
      "Epoch [900/2500], Loss: 0.0000, Accuracy: 98.42276422764228\n",
      "Epoch [1000/2500], Loss: 0.0003, Accuracy: 98.50162601626016\n",
      "Epoch [1100/2500], Loss: 0.0000, Accuracy: 98.56393200295639\n",
      "Epoch [1200/2500], Loss: 0.0000, Accuracy: 98.60907859078591\n",
      "Epoch [1300/2500], Loss: 0.0921, Accuracy: 98.6541588492808\n",
      "Epoch [1400/2500], Loss: 0.0000, Accuracy: 98.69221835075494\n",
      "Epoch [1500/2500], Loss: 0.0000, Accuracy: 98.72195121951219\n",
      "Epoch [1600/2500], Loss: 0.0000, Accuracy: 98.7489837398374\n",
      "Epoch [1700/2500], Loss: 0.0000, Accuracy: 98.7737924438068\n",
      "Epoch [1800/2500], Loss: 0.0000, Accuracy: 98.79358626919603\n",
      "Epoch [1900/2500], Loss: 0.0000, Accuracy: 98.80658964484381\n",
      "Epoch [2000/2500], Loss: 0.0000, Accuracy: 98.8260162601626\n",
      "Epoch [2100/2500], Loss: 0.0753, Accuracy: 98.83894696089818\n",
      "Epoch [2200/2500], Loss: 0.0000, Accuracy: 98.85402808573541\n",
      "Epoch [2300/2500], Loss: 0.0000, Accuracy: 98.86638388123012\n",
      "Epoch [2400/2500], Loss: 0.0000, Accuracy: 98.87872628726288\n",
      "Epoch [2500/2500], Loss: 0.0000, Accuracy: 98.89170731707317\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_sentence = tokenize(pattern)\n",
    "        all_words.extend(tokenized_sentence)\n",
    "        xy.append((tokenized_sentence, tag))\n",
    "\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 512\n",
    "output_size = len(tags)\n",
    "\n",
    "dataset = ChatDataset(intents['intents'], all_words, tags)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "num_epochs = 2500\n",
    "correct=0\n",
    "total=0\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        accuracy =  100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a58e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to chatbot_model_base.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "    }\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76c397",
   "metadata": {},
   "source": [
    "## Deploying Q/A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2c73379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ea958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset JSON\n",
    "with open('piaf-v1.1.json', 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Prétraitement des données\n",
    "paragraphs = []\n",
    "questions = []\n",
    "answers_start = []\n",
    "answers_end = []\n",
    "\n",
    "for article in dataset['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                answer_start = answer['answer_start']\n",
    "                answer_end = answer_start + len(answer['text'])\n",
    "\n",
    "                paragraphs.append(context)\n",
    "                questions.append(question)\n",
    "                answers_start.append(answer_start)\n",
    "                answers_end.append(answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a83aae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle de question-réponse\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "003dfc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer une réponse à partir d'une question\n",
    "def generate_answer(question, context):\n",
    "    # Prétraitement des données\n",
    "    inputs = tokenizer_qa(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Prédiction de la réponse\n",
    "    outputs = model_qa(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Obtenir les indices de début et de fin de la réponse prédite\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    # Convertir les indices en texte\n",
    "    answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index+1]))\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "052b7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour trouver le contexte le plus pertinent pour une question donnée\n",
    "def find_most_relevant_context(question, dataset):\n",
    "    # Utiliser TF-IDF pour représenter les contextes en vecteurs\n",
    "    contexts=[]\n",
    "    for article in dataset['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            contexts += [paragraph['context']]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(contexts)\n",
    "    \n",
    "    # Convertir la question en vecteur\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculer les similarités entre la question et les contextes\n",
    "    similarities = cosine_similarity(question_vector, vectors)\n",
    "    \n",
    "    # Trouver l'index du contexte le plus similaire\n",
    "    most_similar_index = similarities.argmax()\n",
    "    \n",
    "    # Renvoyer le contexte le plus similaire\n",
    "    return contexts[most_similar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05face",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39e2570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: bonjour\n",
      "QueryBot: Content de vous revoir\n",
      "User: ça va?\n",
      "QueryBot: Tout va bien.. Et vous?\n",
      "User: bien\n",
      "QueryBot: Content de le savoir!\n",
      "User: blague\n",
      "QueryBot: Un perfectionniste est entre dans un bar... apparemment, le bar n'etait pas assez haut\n",
      "User: devinett\n",
      "QueryBot: Que trouverez-vous reellement a la fin de chaque arc-en-ciel ?.....La lettre 'w'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDébut de la conversation (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pour finir la conversation...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Saisie utilisateur\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Charger les données et le modèle pré-entraîné\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Charger le modèle\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "# Nom du chatbot\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    # Saisie utilisateur\n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Prédiction de l'intention\n",
    "    sentence_qa = sentence\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    \n",
    "    # Répondre en fonction de l'intention prédite ou en utilisant la question-réponse\n",
    "    if prob.item() > 0.75:  # ajuster le seuil de probabilité selon la précision souhaitée\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                break  # sortir après avoir trouvé une réponse appropriée\n",
    "    else:\n",
    "        response = process_question(sentence_qa)\n",
    "        if response:\n",
    "            print(f\"{bot_name}: {response}\")\n",
    "        else:\n",
    "            context = find_most_relevant_context(sentence_qa, dataset)\n",
    "            answer = generate_answer(sentence_qa, context)\n",
    "            print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47e24097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: Quelle est la dernière nouvelle sur Tunisie Télécom ?\n",
      "QueryBot: ...\n",
      "User: Quelle est la dernière nouvelle sur Tunisie Télécom \n",
      "QueryBot: ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDébut de la conversation (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pour finir la conversation...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from dateutil.parser import parse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Charger les données et le modèle pré-entraîné (à partir de intents.json et du modèle de chatbot)\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Charger le modèle\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "# Nom du chatbot\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "# Fonction de prétraitement du texte\n",
    "def preprocess_text(text):\n",
    "    cleaned_text = text.lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['StageTT']\n",
    "collection = db['TT']\n",
    "\n",
    "# Charger les articles depuis MongoDB et les prétraiter\n",
    "articles = list(collection.find())\n",
    "\n",
    "for article in articles:\n",
    "    article['Content'] = preprocess_text(article['Content'])\n",
    "    if isinstance(article['Publish Date'], str):\n",
    "        try:\n",
    "            article['Publish Date'] = parse(article['Publish Date']).replace(tzinfo=None)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date: {e}, article: {article}\")\n",
    "            article['Publish Date'] = None\n",
    "\n",
    "# Filtrer les articles avec des dates invalides\n",
    "articles = [article for article in articles if article['Publish Date'] is not None]\n",
    "\n",
    "# Categoriser les articles par année et sentiment\n",
    "articles_by_year = defaultdict(lambda: {\"positive\": 0, \"negative\": 0})\n",
    "\n",
    "for article in articles:\n",
    "    year = article['Publish Date'].year\n",
    "    sentiment = article.get('SentimentCamembert', 'positive').lower()\n",
    "    if sentiment == 'positive':\n",
    "        articles_by_year[year]['positive'] += 1\n",
    "    elif sentiment == 'negative':\n",
    "        articles_by_year[year]['negative'] += 1\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    user_input = input(\"User: \").strip().lower()\n",
    "    if user_input == 'quit':\n",
    "        break\n",
    "\n",
    "    # Prédiction de l'intention à l'aide du modèle de chatbot\n",
    "    sentence_qa = user_input\n",
    "    sentence = tokenize(user_input)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    # Répondre en fonction de l'intention prédite\n",
    "    if prob.item() > 0.75:  # ajuster le seuil de probabilité selon la précision souhaitée\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                break  # sortir après avoir trouvé une réponse appropriée\n",
    "    else:\n",
    "        # Répondre en se basant sur les données de la collection MongoDB TT\n",
    "        if \"tunisie télécom\" in user_input:\n",
    "            if articles:\n",
    "                latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "                print(f\"Chatbot: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "            else:\n",
    "                print(\"Chatbot: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "        elif \"combien d'articles\" in user_input and \"positifs\" in user_input:\n",
    "            year_match = re.search(r'\\b\\d{4}\\b', user_input)\n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"Chatbot: Il y a {count} articles positifs en {year}.\")\n",
    "            else:\n",
    "                print(\"Chatbot: Veuillez spécifier une année.\")\n",
    "        elif \"combien d'articles\" in user_input and \"négatifs\" in user_input:\n",
    "            year_match = re.search(r'\\b\\d{4}\\b', user_input)\n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"Chatbot: Il y a {count} articles négatifs en {year}.\")\n",
    "            else:\n",
    "                print(\"Chatbot: Veuillez spécifier une année.\")\n",
    "        elif \"année avec le plus d'articles négatifs\" in user_input:\n",
    "            year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "            count = articles_by_year[year]['negative']\n",
    "            print(f\"Chatbot: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "        elif \"année avec le plus d'articles positifs\" in user_input:\n",
    "            year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "            count = articles_by_year[year]['positive']\n",
    "            print(f\"Chatbot: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            # Aucune intention prédite, ni réponse directe trouvée\n",
    "            print(\"Chatbot: Désolé, je ne comprends pas votre question.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fcfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
