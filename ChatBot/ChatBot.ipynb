{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a91d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d8ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, intents, all_words, tags):\n",
    "        self.intents = intents\n",
    "        self.all_words = all_words\n",
    "        self.tags = tags\n",
    "        self.xy = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        xy = []\n",
    "        for intent in self.intents:\n",
    "            tag = intent['tag']\n",
    "            for pattern in intent['patterns']:\n",
    "                tokenized_sentence = tokenize(pattern)\n",
    "                bag = bag_of_words(tokenized_sentence, self.all_words)\n",
    "                label = self.tags.index(tag)\n",
    "                xy.append((bag, label))\n",
    "        return xy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bag, label = self.xy[index]\n",
    "        return bag, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout avec taux de 0.5\n",
    "\n",
    "        # Initialisation des poids avec la méthode de He\n",
    "        torch.nn.init.kaiming_uniform_(self.l1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la première couche cachée\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout après la deuxième couche cachée\n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa7c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2500], Loss: 0.0743, Accuracy: 92.48780487804878\n",
      "Epoch [200/2500], Loss: 0.0019, Accuracy: 95.82926829268293\n",
      "Epoch [300/2500], Loss: 0.0003, Accuracy: 96.92682926829268\n",
      "Epoch [400/2500], Loss: 0.0003, Accuracy: 97.48373983739837\n",
      "Epoch [500/2500], Loss: 0.0001, Accuracy: 97.82764227642276\n",
      "Epoch [600/2500], Loss: 0.0000, Accuracy: 98.06233062330624\n",
      "Epoch [700/2500], Loss: 0.0001, Accuracy: 98.20673635307782\n",
      "Epoch [800/2500], Loss: 0.0000, Accuracy: 98.32825203252033\n",
      "Epoch [900/2500], Loss: 0.0002, Accuracy: 98.42005420054201\n",
      "Epoch [1000/2500], Loss: 0.0000, Accuracy: 98.4910569105691\n",
      "Epoch [1100/2500], Loss: 0.0000, Accuracy: 98.54249815225425\n",
      "Epoch [1200/2500], Loss: 0.0000, Accuracy: 98.59349593495935\n",
      "Epoch [1300/2500], Loss: 0.0000, Accuracy: 98.63727329580988\n",
      "Epoch [1400/2500], Loss: 0.0000, Accuracy: 98.66898954703832\n",
      "Epoch [1500/2500], Loss: 0.0000, Accuracy: 98.69539295392954\n",
      "Epoch [1600/2500], Loss: 0.0000, Accuracy: 98.72103658536585\n",
      "Epoch [1700/2500], Loss: 0.0000, Accuracy: 98.74605451936873\n",
      "Epoch [1800/2500], Loss: 0.0000, Accuracy: 98.77235772357723\n",
      "Epoch [1900/2500], Loss: 0.0000, Accuracy: 98.79375267436885\n",
      "Epoch [2000/2500], Loss: 0.0000, Accuracy: 98.81138211382114\n",
      "Epoch [2100/2500], Loss: 0.0695, Accuracy: 98.82655826558266\n",
      "Epoch [2200/2500], Loss: 0.0000, Accuracy: 98.83776792313378\n",
      "Epoch [2300/2500], Loss: 0.0638, Accuracy: 98.8582537999293\n",
      "Epoch [2400/2500], Loss: 0.0000, Accuracy: 98.87330623306234\n",
      "Epoch [2500/2500], Loss: 0.0000, Accuracy: 98.88520325203253\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_sentence = tokenize(pattern)\n",
    "        all_words.extend(tokenized_sentence)\n",
    "        xy.append((tokenized_sentence, tag))\n",
    "\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 512\n",
    "output_size = len(tags)\n",
    "\n",
    "dataset = ChatDataset(intents['intents'], all_words, tags)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "num_epochs = 2500\n",
    "correct=0\n",
    "total=0\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        accuracy =  100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a58e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to chatbot_model_base.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "    }\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76c397",
   "metadata": {},
   "source": [
    "## Deploying Q/A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c73379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ea958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset JSON\n",
    "with open('piaf-v1.1.json', 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Prétraitement des données\n",
    "paragraphs = []\n",
    "questions = []\n",
    "answers_start = []\n",
    "answers_end = []\n",
    "\n",
    "for article in dataset['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                answer_start = answer['answer_start']\n",
    "                answer_end = answer_start + len(answer['text'])\n",
    "\n",
    "                paragraphs.append(context)\n",
    "                questions.append(question)\n",
    "                answers_start.append(answer_start)\n",
    "                answers_end.append(answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74c291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Mega-Pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle de question-réponse\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0bedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer une réponse à partir d'une question\n",
    "def generate_answer(question, context):\n",
    "    # Prétraitement des données\n",
    "    inputs = tokenizer_qa(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Prédiction de la réponse\n",
    "    outputs = model_qa(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Obtenir les indices de début et de fin de la réponse prédite\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    # Convertir les indices en texte\n",
    "    answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index+1]))\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f686d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour trouver le contexte le plus pertinent pour une question donnée\n",
    "def find_most_relevant_context(question, dataset):\n",
    "    # Utiliser TF-IDF pour représenter les contextes en vecteurs\n",
    "    contexts=[]\n",
    "    for article in dataset['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            contexts += [paragraph['context']]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(contexts)\n",
    "    \n",
    "    # Convertir la question en vecteur\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculer les similarités entre la question et les contextes\n",
    "    similarities = cosine_similarity(question_vector, vectors)\n",
    "    \n",
    "    # Trouver l'index du contexte le plus similaire\n",
    "    most_similar_index = similarities.argmax()\n",
    "    \n",
    "    # Renvoyer le contexte le plus similaire\n",
    "    return contexts[most_similar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05face",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8deac88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=146, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l3): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- IMPORT LIBRARY -----------------\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ------ IMPORT DATA & MODEL FOR BASE CASE ------\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdd2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(text):\n",
    "    cleaned_text = text.lower().strip() \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbada49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la conversation ('quit' pour finir la conversation...)\n",
      "User: bonjour\n",
      "QueryBot: Bonjour\n",
      "User: ça va?\n",
      "QueryBot: Tout va bien.. Et vous?\n",
      "User: bien\n",
      "QueryBot: Content de le savoir!\n",
      "User: blague \n",
      "QueryBot: Comment appelle-t-on un bonhomme de neige avec un bronzage? Une flaque.\n",
      "User: devinette\n",
      "QueryBot: Pourquoi un velo ne peut-il pas tenir tout seul ?.....Il est trop fatigue.\n",
      "User: combien d'articles positifs en 2022\n",
      "QueryBot: Il y a 18 articles positifs en 2022.\n",
      "User: combien d'articles négatifs en 2022\n",
      "QueryBot: Il y a 3 articles négatifs en 2022.\n",
      "User: tt\n",
      "QueryBot: Tunisie Telecom est l'operateur historique de telecommunications en Tunisie. Fondee en 1996, elle fournit une gamme de services de telephonie fixe et mobile, d'acces a Internet et d'autres services de telecommunication a travers le pays. Tunisie Telecom joue un role crucial dans l'infrastructure de communication nationale et dans le developpement des technologies de l'information et de la communication en Tunisie.\n",
      "User: quelle est la dernieère nouvelle sur tt\n",
      "QueryBot: ...\n",
      "User: quelle est la dernière nouvelle sur tunisie télécom\n",
      "QueryBot: La dernière nouvelle sur Tunisie Télécom est : Tunisie Télécom et l’AtGmo : “Run of Heroes” pour soutenir les malades d’hémopathies\n",
      "User: tunisie télécom\n",
      "QueryBot: La dernière nouvelle sur Tunisie Télécom est : Tunisie Télécom et l’AtGmo : “Run of Heroes” pour soutenir les malades d’hémopathies\n",
      "User: donne moi un article positif de 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Voici un article positive pour l'année 2024: Tunisie Télécom et l’AtGmo : “Run of Heroes” pour soutenir les malades d’hémopathies\n",
      "Résumé:  Tunisie Télécom, partenaire of l’association tunisienne des greffés de la moelle osseuse . L’AtGmo peut assurer l'hébergement de 126 malades et accompagnateurs .\n",
      "User: un article négatif de 2019\n",
      "QueryBot: ...\n",
      "User: donne moi un article négatif de 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your max_length is set to 150, but your input_length is only 95. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryBot: Voici un article negative pour l'année 2019: Frais élevés pour des services de qualité inférieure chez Tunisie Télécom\n",
      "Résumé:  Tunisie Télécom facturés facturé des services de qualité inférieure suscitent des critiques parmi les abonnesnes .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDébut de la conversation (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pour finir la conversation...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import pymongo\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"`resume_download` is deprecated and will be removed in version 1.0.0\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Fonction pour résumer un article\n",
    "def summarize_article(content):\n",
    "    summarizer = pipeline(\"summarization\", min_length=30, max_length=90)\n",
    "    max_tokens = 1024  # Longueur maximale du modèle à ajuster si nécessaire\n",
    "    chunks = [content[i:i + max_tokens] for i in range(0, len(content), max_tokens)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Fonction pour récupérer les articles depuis MongoDB\n",
    "def fetch_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "    articles = list(collection.find({}))\n",
    "    \n",
    "    for article in articles:\n",
    "        if isinstance(article['Publish Date'], str):\n",
    "            try:\n",
    "                parsed_date = parser.parse(article['Publish Date'])\n",
    "                article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "\n",
    "# Chargement des intentions depuis le fichier JSON\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "# Chargement du modèle de chatbot\n",
    "FILE = \"chatbot_model_base.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "# Nom du bot\n",
    "bot_name = \"QueryBot\"\n",
    "\n",
    "print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "while True:\n",
    "    sentence = input(\"User: \")\n",
    "    if sentence == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Intent pour récupérer la dernière nouvelle sur Tunisie Télécom\n",
    "    if \"tunisie télécom\" in sentence.lower():\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "    # Intent pour gérer les questions sur les articles positifs/négatifs par année\n",
    "    elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            articles_by_year = {}\n",
    "            for article in articles:\n",
    "                year = article['Publish Date'].year\n",
    "                sentiment = article['SentimentCamembert']\n",
    "                if year not in articles_by_year:\n",
    "                    articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                if sentiment == 'positive':\n",
    "                    articles_by_year[year]['positive'] += 1\n",
    "                elif sentiment == 'negative':\n",
    "                    articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            if \"combien d'articles positifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(0))\n",
    "                    negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                count = articles_by_year[year]['negative']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                count = articles_by_year[year]['positive']\n",
    "                print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    # Intent pour récupérer un article positif ou négatif pour une certaine année\n",
    "    elif \"donne moi un article positif\" in sentence.lower() or \"donne moi un article négatif\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = 'positive' if \"positif\" in sentence.lower() else 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            filtered_articles = [article for article in articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "            \n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in filtered_articles if article['Publish Date'].year == year]\n",
    "            \n",
    "            if filtered_articles:\n",
    "                selected_article = random.choice(filtered_articles)\n",
    "                summary = summarize_article(selected_article['Content'])\n",
    "                print(f\"{bot_name}: Voici un article {sentiment_filter} pour l'année {selected_article['Publish Date'].year}: {selected_article['Title']}\\nRésumé: {summary}\")\n",
    "            else:\n",
    "                print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour cette année.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Intent pour résumer tous les articles positifs/négatifs d'une certaine année\n",
    "    elif \"résume moi tous les articles positifs\" in sentence.lower() or \"résume moi tous les articles négatifs\" in sentence.lower() or \"résume moi tous les articles de\" in sentence.lower():\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        sentiment_filter = None\n",
    "        if \"positifs\" in sentence.lower():\n",
    "            sentiment_filter = 'positive'\n",
    "        elif \"négatifs\" in sentence.lower():\n",
    "            sentiment_filter = 'negative'\n",
    "        \n",
    "        articles = fetch_articles()\n",
    "        if articles:\n",
    "            if year_match:\n",
    "                year = int(year_match.group(0))\n",
    "                filtered_articles = [article for article in articles if article['Publish Date'].year == year]\n",
    "                if sentiment_filter:\n",
    "                    filtered_articles = [article for article in filtered_articles if article['SentimentCamembert'] == sentiment_filter]\n",
    "                \n",
    "                if filtered_articles:\n",
    "                    for article in filtered_articles:\n",
    "                        summary = summarize_article(article['Content'])\n",
    "                        print(f\"{bot_name}: {article['Title']}\\nRésumé: {summary}\\n\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé pour l'année {year}.\")\n",
    "            else:\n",
    "                filtered_articles = [article for article in articles if sentiment_filter is None or article['SentimentCamembert'] == sentiment_filter]\n",
    "                if filtered_articles:\n",
    "                    for article in filtered_articles:\n",
    "                        summary = summarize_article(article['Content'])\n",
    "                        print(f\"{bot_name}: {article['Title']}\\nRésumé: {summary}\\n\")\n",
    "                else:\n",
    "                    print(f\"{bot_name}: Aucun article {sentiment_filter} trouvé.\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Aucun article trouvé dans la base de données.\")\n",
    "    \n",
    "    # Si aucune intention spécifique n'est détectée, utiliser le modèle de classification de questions\n",
    "    else:\n",
    "        sentence_qa = sentence\n",
    "        # Utiliser des fonctions appropriées pour tokenizer et transformer la phrase en vecteurs\n",
    "        sentence = tokenize(sentence)\n",
    "        X = bag_of_words(sentence, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.75:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent['tag']:\n",
    "                    response = random.choice(intent['responses'])\n",
    "                    print(f\"{bot_name}: {response}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: Je ne suis pas sûr de comprendre votre demande. Pouvez-vous reformuler?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#import json\n",
    "#import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Charger les données et le modèle pré-entraîné\n",
    "#with open('intents.json', 'r') as json_data:\n",
    "#   intents = json.load(json_data)\n",
    "\n",
    "#FILE = \"chatbot_model_base.pth\"\n",
    "#data = torch.load(FILE)\n",
    "\n",
    "#input_size = data[\"input_size\"]\n",
    "#hidden_size = data[\"hidden_size\"]\n",
    "#output_size = data[\"output_size\"]\n",
    "#all_words = data['all_words']\n",
    "#tags = data['tags']\n",
    "#model_state = data[\"model_state\"]\n",
    "\n",
    "# Charger le modèle\n",
    "#model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model.load_state_dict(model_state)\n",
    "#model.eval()\n",
    "\n",
    "# Nom du chatbot\n",
    "#bot_name = \"QueryBot\"\n",
    "\n",
    "#print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "#while True:\n",
    "    # Saisie utilisateur\n",
    "    #sentence = input(\"User: \")\n",
    "    #if sentence == 'quit':\n",
    "     #   break\n",
    "    \n",
    "    # Prédiction de l'intention\n",
    "   # sentence_qa = sentence\n",
    "   # sentence = tokenize(sentence)\n",
    "   # X = bag_of_words(sentence, all_words)\n",
    "    #X = X.reshape(1, X.shape[0])\n",
    "    #X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    #output = model(X)\n",
    "    #_, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    #tag = tags[predicted.item()]\n",
    "\n",
    "    #probs = torch.softmax(output, dim=1)\n",
    "    #prob = probs[0][predicted.item()]\n",
    "    \n",
    "    # Répondre en fonction de l'intention prédite ou en utilisant la question-réponse\n",
    "    #if prob.item() > 0.75:  # ajuster le seuil de probabilité selon la précision souhaitée\n",
    "     #   for intent in intents['intents']:\n",
    "      #      if tag == intent[\"tag\"]:\n",
    "       #         print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        #        break  # sortir après avoir trouvé une réponse appropriée\n",
    "   # else:\n",
    "    #    response = process_question(sentence_qa)\n",
    "     #   if response:\n",
    "      #      print(f\"{bot_name}: {response}\")\n",
    "       # else:\n",
    "        #    context = find_most_relevant_context(sentence_qa, dataset)\n",
    "         #   answer = generate_answer(sentence_qa, context)\n",
    "          #  print(f\"{bot_name}: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User: donne moi un article positif de 2023\n",
    "#QueryBot: Voici un article positif pour l'année 2023: Title Article Positif 2023\n",
    "#User: combien d'articles positifs en 2022\n",
    "#QueryBot: Il y a 3 articles positifs en 2022.\n",
    "#User: année avec le plus d'articles négatifs\n",
    "#QueryBot: L'année avec le plus d'articles négatifs est 2021 avec 5 articles.\n",
    "#User: tunisie télécom\n",
    "#QueryBot: La dernière nouvelle sur Tunisie Télécom est : Title Dernière Nouvelle\n",
    "#User: donne moi un article négatif\n",
    "#QueryBot: Voici un article négatif pour l'année 2023: Title Article Négatif 2023\n",
    "#User: raconte-moi une blague\n",
    "#QueryBot: Désolé, je n'ai pas compris votre question. Pouvez-vous reformuler ?\n",
    "#User: quit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b336d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#import json\n",
    "#import torch\n",
    "#import pymongo\n",
    "#import re\n",
    "#from dateutil import parser\n",
    "#from datetime import datetime\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#def fetch_articles():\n",
    "    #client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    #db = client[\"StageTT\"]\n",
    "    #collection = db[\"TT\"]\n",
    "    #articles = list(collection.find({}))\n",
    "    \n",
    "  \n",
    "    #for article in articles:\n",
    "        #if isinstance(article['Publish Date'], str):\n",
    "            #try:\n",
    "               \n",
    "                #parsed_date = parser.parse(article['Publish Date'])\n",
    "               \n",
    "                #article['Publish Date'] = parsed_date.replace(tzinfo=None)\n",
    "            #except ValueError:\n",
    "                #print(f\"Warning: Failed to parse date for article {article['_id']}\")\n",
    "    \n",
    "    #return articles\n",
    "\n",
    "\n",
    "#with open('intents.json', 'r') as json_data:\n",
    "    #intents = json.load(json_data)\n",
    "\n",
    "#FILE = \"chatbot_model_base.pth\"\n",
    "#data = torch.load(FILE)\n",
    "\n",
    "#input_size = data[\"input_size\"]\n",
    "#hidden_size = data[\"hidden_size\"]\n",
    "#output_size = data[\"output_size\"]\n",
    "#all_words = data['all_words']\n",
    "#tags = data['tags']\n",
    "#model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "#model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model.load_state_dict(model_state)\n",
    "#model.eval()\n",
    "\n",
    "\n",
    "#bot_name = \"QueryBot\"\n",
    "\n",
    "#print(\"Début de la conversation ('quit' pour finir la conversation...)\")\n",
    "#while True:\n",
    "  \n",
    "    #sentence = input(\"User: \")\n",
    "    #if sentence == 'quit':\n",
    "        #break\n",
    "    \n",
    "  \n",
    "    #if \"tunisie télécom\" in sentence.lower():\n",
    "        #articles = fetch_articles()\n",
    "        #if articles:\n",
    "            #latest_article = max(articles, key=lambda x: x['Publish Date'])\n",
    "            #print(f\"{bot_name}: La dernière nouvelle sur Tunisie Télécom est : {latest_article['Title']}\")\n",
    "        #else:\n",
    "            #print(f\"{bot_name}: Il n'y a pas d'articles disponibles sur Tunisie Télécom.\")\n",
    "    \n",
    "\n",
    "    #elif any(word in sentence.lower() for word in [\"combien d'articles positifs\", \"combien d'articles négatifs\", \"année avec le plus d'articles négatifs\", \"année avec le plus d'articles positifs\"]):\n",
    "        #year_match = re.search(r'\\b\\d{4}\\b', sentence)\n",
    "        #articles = fetch_articles()\n",
    "        #if articles:\n",
    "            #articles_by_year = {}\n",
    "            #for article in articles:\n",
    "                #year = article['Publish Date'].year\n",
    "                #sentiment = article['SentimentCamembert']\n",
    "                #if year not in articles_by_year:\n",
    "                    #articles_by_year[year] = {'positive': 0, 'negative': 0}\n",
    "                #if sentiment == 'positive':\n",
    "                    #articles_by_year[year]['positive'] += 1\n",
    "                #elif sentiment == 'negative':\n",
    "                    #articles_by_year[year]['negative'] += 1\n",
    "            \n",
    "            #if \"combien d'articles positifs\" in sentence.lower():\n",
    "                #if year_match:\n",
    "                    #year = int(year_match.group(0))\n",
    "                    #positive_count = articles_by_year.get(year, {}).get('positive', 0)\n",
    "                    #print(f\"{bot_name}: Il y a {positive_count} articles positifs en {year}.\")\n",
    "                #else:\n",
    "                    #print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles positifs.\")\n",
    "            #elif \"combien d'articles négatifs\" in sentence.lower():\n",
    "                #if year_match:\n",
    "                    #year = int(year_match.group(0))\n",
    "                    #negative_count = articles_by_year.get(year, {}).get('negative', 0)\n",
    "                    #print(f\"{bot_name}: Il y a {negative_count} articles négatifs en {year}.\")\n",
    "                #else:\n",
    "                    #print(f\"{bot_name}: Veuillez spécifier une année valide pour obtenir le nombre d'articles négatifs.\")\n",
    "            #elif \"année avec le plus d'articles négatifs\" in sentence.lower():\n",
    "                #year = max(articles_by_year, key=lambda x: articles_by_year[x]['negative'])\n",
    "                #count = articles_by_year[year]['negative']\n",
    "                #print(f\"{bot_name}: L'année avec le plus d'articles négatifs est {year} avec {count} articles.\")\n",
    "            #elif \"année avec le plus d'articles positifs\" in sentence.lower():\n",
    "                #year = max(articles_by_year, key=lambda x: articles_by_year[x]['positive'])\n",
    "                #count = articles_by_year[year]['positive']\n",
    "                #print(f\"{bot_name}: L'année avec le plus d'articles positifs est {year} avec {count} articles.\")\n",
    "        #else:\n",
    "            #print(f\"{bot_name}: Aucun article trouvé.\")\n",
    "    \n",
    "    \n",
    "    #else:\n",
    "        #sentence_qa = sentence\n",
    "        #sentence = tokenize(sentence)\n",
    "        #X = bag_of_words(sentence, all_words)\n",
    "        #X = X.reshape(1, X.shape[0])\n",
    "        #X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        #output = model(X)\n",
    "        #_, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        #tag = tags[predicted.item()]\n",
    "\n",
    "        #probs = torch.softmax(output, dim=1)\n",
    "        #prob = probs[0][predicted.item()]\n",
    "\n",
    "        \n",
    "        #if prob.item() > 0.75:  \n",
    "            #for intent in intents['intents']:\n",
    "                #if tag == intent[\"tag\"]:\n",
    "                    #print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                    #break\n",
    "        #else:\n",
    "            #response = process_question(sentence_qa)\n",
    "            #if response:\n",
    "                #print(f\"{bot_name}: {response}\")\n",
    "            #else:\n",
    "                #context = find_most_relevant_context(sentence_qa, dataset)\n",
    "                #answer = generate_answer(sentence_qa, context)\n",
    "                #print(f\"{bot_name}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dc6eb",
   "metadata": {},
   "source": [
    "## Summary Model for the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26605906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_orangesum = load_dataset(\"GEM/OrangeSum\", \"abstract\") # we can also specify \"title\" to obtain pairs of text-title\n",
    "#dataset_xlsum = load_dataset(\"csebuetnlp/xlsum\", \"french\")\n",
    "#dataset_mlsum = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2bd8d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e37c497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"french\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a76d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (21401, 4), 'test': (1500, 4), 'validation': (1500, 4)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orangesum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0e798af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gem_id        0\n",
       "input         0\n",
       "target        0\n",
       "references    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_OS = pd.DataFrame(dataset_orangesum['test'])\n",
    "df_train_OS = pd.DataFrame(dataset_orangesum['train'])\n",
    "df_validation_OS = pd.DataFrame(dataset_orangesum['validation'])\n",
    "\n",
    "df_train_OS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "121f7183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gem_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OrangeSum_abstract-train-0</td>\n",
       "      <td>Thierry Mariani sur la liste du Rassemblement ...</td>\n",
       "      <td>L'information n'a pas été confirmée par l'inté...</td>\n",
       "      <td>[L'information n'a pas été confirmée par l'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OrangeSum_abstract-train-1</td>\n",
       "      <td>C'est désormais officiel : Alain Juppé n'est p...</td>\n",
       "      <td>Le maire de Bordeaux ne fait plus partie des R...</td>\n",
       "      <td>[Le maire de Bordeaux ne fait plus partie des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OrangeSum_abstract-train-2</td>\n",
       "      <td>La mesure est décriée par les avocats et les m...</td>\n",
       "      <td>En 2020, les tribunaux d'instance fusionnent a...</td>\n",
       "      <td>[En 2020, les tribunaux d'instance fusionnent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OrangeSum_abstract-train-3</td>\n",
       "      <td>Dans une interview accordée au Figaro mercredi...</td>\n",
       "      <td>Les médecins jugés \"gros prescripteurs d'arrêt...</td>\n",
       "      <td>[Les médecins jugés \"gros prescripteurs d'arrê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OrangeSum_abstract-train-4</td>\n",
       "      <td>Le préjudice est estimé à 2 millions d'euros. ...</td>\n",
       "      <td>Il aura fallu mobiliser 90 gendarmes pour cett...</td>\n",
       "      <td>[Il aura fallu mobiliser 90 gendarmes pour cet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       gem_id  \\\n",
       "0  OrangeSum_abstract-train-0   \n",
       "1  OrangeSum_abstract-train-1   \n",
       "2  OrangeSum_abstract-train-2   \n",
       "3  OrangeSum_abstract-train-3   \n",
       "4  OrangeSum_abstract-train-4   \n",
       "\n",
       "                                               input  \\\n",
       "0  Thierry Mariani sur la liste du Rassemblement ...   \n",
       "1  C'est désormais officiel : Alain Juppé n'est p...   \n",
       "2  La mesure est décriée par les avocats et les m...   \n",
       "3  Dans une interview accordée au Figaro mercredi...   \n",
       "4  Le préjudice est estimé à 2 millions d'euros. ...   \n",
       "\n",
       "                                              target  \\\n",
       "0  L'information n'a pas été confirmée par l'inté...   \n",
       "1  Le maire de Bordeaux ne fait plus partie des R...   \n",
       "2  En 2020, les tribunaux d'instance fusionnent a...   \n",
       "3  Les médecins jugés \"gros prescripteurs d'arrêt...   \n",
       "4  Il aura fallu mobiliser 90 gendarmes pour cett...   \n",
       "\n",
       "                                          references  \n",
       "0  [L'information n'a pas été confirmée par l'int...  \n",
       "1  [Le maire de Bordeaux ne fait plus partie des ...  \n",
       "2  [En 2020, les tribunaux d'instance fusionnent ...  \n",
       "3  [Les médecins jugés \"gros prescripteurs d'arrê...  \n",
       "4  [Il aura fallu mobiliser 90 gendarmes pour cet...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_OS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0def09a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mega-\n",
      "[nltk_data]     Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "# Clean the summaries and texts\n",
    "clean_target = []\n",
    "for target in df_train_OS.target:\n",
    "    clean_target.append(clean_text(target, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_input = []\n",
    "for input in df_train_OS.input:\n",
    "    clean_input.append(clean_text(input))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f2cc04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean News # 1\n",
      "l information n a pas été confirmée par l intéressé qui déclare toutefois étudier la question \n",
      "\n",
      "thierry mariani liste rassemblement national rn ex fn européennes affirme mardi 11 septembre chez pol nouvelle newsletter politique libération ancien député républicain ministre nicolas sarkozy point rejoindre troupes marine pen élections européennes 2019 ça va faire plus question calendrier obligé annoncer tout suite huit mois européennes ainsi assuré membre influent rn contacté franceinfo mariani a confirmé information élections juin sais numéro 1 liste a répondu ancien ministre transports reconnaît toutefois toujours cité franceinfo nom liste rn fait partie possibilités fréjus ville sympathique prévu rendre week end a ailleurs commenté twitter alors marine pen réunit cadres parti week end cité varoise proximité connue fnla proximité thierry mariani parti frontiste nouvelle sans alliés allons rester opposition longtemps temps renverser table front national a évolué regardons si accord rapprochement possibles déclaré interview donnée journal dimanche mars dernier puis avril bruissé rumeur rencontre entre ex député marine pen proposé figurer position éligible liste parti européennes conclusion hâtive époque écrit twitter mois thierry mariani cosigné tribune publiée valeurs actuelles côté élus frontistes appelant union droites\n",
      "\n",
      "Clean News # 2\n",
      "le maire de bordeaux ne fait plus partie des républicains et il tient à le montrer  lors de ses voeux à la presse  l édile a pris ses distances avec sa famille politique historique  qu il trouve trop proche de marine le pen \n",
      "\n",
      "désormais officiel alain juppé plus membre républicains ex premier ministre jacques chirac cofondateur ump 2002 paie plus cotisation auprès parti droite mercredi 9 janvier maire bordeaux a dénoncé glissement opère selon droite vers extême droite reconnais moins moins cette famille politique laquelle pourtant très attaché tristesse quittée a dérive vers thèses celles très proches extrême droite ambiguïté europe a déclaré face journalistes réunis assister voeux assiste cette espèce transfusion régulière thèmes fond a moments où demande entends radio membre lr rn a insisté maire bordeaux jour ex député thierry mariani annonçait départ lr rallier liste rassemblement national européennes mai prochain cela fait deux ans dit prenais distances républicains choses acquises depuis bien longtemps a tranché alain juppé ancien candidat primaire droite présidentielle 2017 a jamais fait mystère désaccord positions président républicains laurent wauquiez\n",
      "\n",
      "Clean News # 3\n",
      "en 2020  les tribunaux d instance fusionnent avec ceux de grande instance pour former un unique  tribunal judiciaire   c est la principale mesure de la réforme de la justice  portée par la garde des sceaux nicole belloubet \n",
      "\n",
      "mesure décriée avocats magistrats juridictions proximité excellence traitant litiges quotidien tribunaux instance apprêtent fusionner tribunaux grande instance cette réorganisation principales mesures réforme justice promulguée 23 mars professionnels inquiètent dévitalisation petites juridictions accès plus restreint juge réforme justice pourquoi avocats magistrats greffe colère dauphiné libérédepuis 1958 tribunaux instance ti tribunaux grande instance tgi partageaient contentieux civils selon répartition essentiellement fondée montant litige héritiers juges paix juges instance surnommés juges pauvres tranchaient toutes affaires lesquelles demande portait inférieures 10 000 euros expulsions locatives dettes impayées passant travaux mal exécutés conflits liés accidents circulation également compétents tutelles 1er janvier 285 tribunaux instance disparaissent ainsi 164 tgi france recours accru procédures dématérialisées quand tribunal instance situé commune tgi 57 ti concernés cette situation fusionnent former tribunal judiciaire quand ti situé commune différente comme ivry seine val marne condom gers molsheim bas rhin devient chambre détachée tribunal judiciaire appelé tribunal proximité alors particuliers pouvaient présenter directement greffe tribunal instance déposer requête réforme renforce recours accru procédures dématérialisées étend représentation obligatoire avocat exit aussi juge instance appellera désormais juge contentieux protection restera magistrat spécialisé affaires liées vulnérabilités économiques sociales garde sceaux dû renoncer supprimer cette fonction statutaire devant bronca opposants réforme compétences ajoutés décret quid leurs compétences deux principaux syndicats magistrats dénoncent flou autour question loi facilite création pôles spécialisés départements plusieurs tribunaux grande instance permet attribuer compétences supplémentaires tribunaux proximité mieux adapter besoins particuliers territoires souligne ministère justice ajouts compétences spécialisations décidés décrets après propositions chefs cours appel sans calendrier précis relèvent union syndicale magistrats usm syndicat magistrature sm plus gros bouleversement beaucoup cabinets juges instruction vont être supprimés juges application peines sait a aucune visibilité tacle présidente usm céline parisot calculs électoraux collègues savent chefs cour proposé ministre abonde katia dubreuil présidente sm déplorant absence concertation étonnée gouvernement souhaité différer annonces selon résultats électoraux république marche communes concernées comme écrivait canard enchaîné série articles fin octobre empêtrée cette polémique ministre bornée défendre toute partialité déplorant fusion conduite aveugle précipitation syndicats magistrats voient volonté faire économies échelle mutualisant effectifs greffes tribunaux aussi tribunaux conseils prud hommes malgré insistance garde sceaux répéter tous sites maintenus syndicats redoutent cette réforme prélude refonte carte judiciaire où tribunaux proximité vidés substance finiraient fermer depuis annonce cette fusion a près deux ans garde sceaux nicole belloubet invoque nécessité simplification lisibilité porte entrée unique justice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(3):\n",
    "    print(\"Clean News #\",i+1)\n",
    "    print(clean_target[i])\n",
    "    print(clean_input[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7920f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d15cbb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 111529\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_target)\n",
    "count_words(word_counts, clean_input)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "354ece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "from torchtext.vocab import Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "235736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load of pretrained embeddings vectors :\n",
    "pretrained_vectors_fasttext = FastText(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a073f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained vocabulary contains 1152449 words, with embeddings in vectors of size 300\n"
     ]
    }
   ],
   "source": [
    "print(f'The pre-trained vocabulary contains {pretrained_vectors_fasttext.vectors.shape[0]} words, with embeddings in vectors of size {pretrained_vectors_fasttext.vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22f707b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors_words = pretrained_vectors_fasttext.stoi.keys()\n",
    "pretrained_vectors_values = pretrained_vectors_fasttext.stoi.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "311cebf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1663, -0.2434, -0.1298,  0.2558,  0.2620,  0.4340,  0.2389,  0.2846,\n",
       "        -0.0285,  0.2952, -0.1806, -0.0203,  0.2095,  0.2392,  0.4044,  0.2178,\n",
       "         0.3261,  0.1015,  0.1417, -0.1413, -0.1626, -0.6919, -0.1303,  0.5766,\n",
       "         0.2136, -0.0434, -0.4864,  0.2376, -0.3875,  0.0248,  0.5002,  0.4109,\n",
       "        -0.2349,  0.2109, -0.1231, -0.1220, -0.2864, -0.2508, -0.2469,  0.0470,\n",
       "         0.2941, -0.2932, -0.0470, -0.0928,  0.0722, -0.0158,  0.2090,  0.1393,\n",
       "         0.3059,  0.3177, -0.1812, -0.0239, -0.1266,  0.0802, -0.1903, -0.2608,\n",
       "        -0.3757, -0.0703,  0.3611,  0.2268, -0.1355,  0.2499, -0.0559, -0.1626,\n",
       "         0.1937,  0.3333, -0.0398, -0.0106, -0.2556, -0.2036,  0.3537,  0.0297,\n",
       "         0.0255, -0.1837,  0.1164, -0.3757,  0.2895, -0.2726,  0.0061, -0.2071,\n",
       "        -0.2901, -0.0297, -0.0647,  0.1851, -0.0209, -0.0855,  0.0574,  0.3292,\n",
       "        -0.3409, -0.4960, -0.1257, -0.3342,  0.0513, -0.0179,  0.0588,  0.0645,\n",
       "        -0.2976,  0.0638,  0.2410, -0.2550, -0.2986, -0.2085, -0.2570,  0.4697,\n",
       "         0.0947,  0.3796,  0.2746, -0.4441,  0.2341,  0.0768, -0.0301, -0.0469,\n",
       "         0.1435,  0.2824,  0.2580, -0.2594,  0.2596, -0.0618, -0.0595, -0.1947,\n",
       "         0.3589, -0.1353, -0.4342, -0.2739, -0.0385,  0.1818,  0.1305, -0.1849,\n",
       "         0.1211, -0.0591, -0.1312,  0.1912,  0.3404, -0.0382,  0.1397, -0.0051,\n",
       "        -0.2034,  0.0856, -0.0016,  0.0147,  0.0628,  0.1353, -0.2379,  0.1144,\n",
       "         0.3324, -0.2130, -0.1240,  0.2746,  0.5219,  0.4409, -0.2259,  0.0107,\n",
       "        -0.0836,  0.2050,  0.1743, -0.0184,  0.1690,  0.1539,  0.0077,  0.4725,\n",
       "        -0.3211, -0.0922, -0.1920, -0.0969, -0.3629,  0.2679,  0.2557, -0.0857,\n",
       "        -0.1151, -0.1501, -0.1330,  0.0605,  0.0886, -0.0343, -0.0945, -0.1892,\n",
       "         0.0479, -0.1171,  0.2026, -0.1340, -0.0248,  0.1926,  0.5280, -0.2757,\n",
       "         0.3417, -0.0139,  0.0388, -0.1377, -0.2876, -0.5761, -0.1273, -0.2940,\n",
       "         0.1769,  0.2692,  0.1415, -0.1625,  0.0234,  0.0058, -0.5362, -0.1814,\n",
       "         0.6860, -0.0921,  0.2931,  0.0845, -0.5805, -0.1804, -0.0793,  0.2759,\n",
       "        -0.1169,  0.6628,  0.1200,  0.2342, -0.0388, -0.1099,  0.0519,  0.1616,\n",
       "        -0.0277, -0.2439,  0.0415,  0.1379,  0.0974, -0.1609, -0.0360, -0.1602,\n",
       "         0.1139, -0.1470, -0.0983,  0.1412,  0.2697, -0.1490,  0.1723, -0.2127,\n",
       "         0.1073,  0.3893, -0.1609,  0.1174,  0.4289, -0.1584,  0.1251,  0.4409,\n",
       "        -0.1356,  0.0074, -0.5189, -0.0558,  0.0380, -0.1450, -0.0888, -0.2765,\n",
       "         0.3083,  0.1420, -0.0556,  0.3881, -0.0875,  0.0239, -0.1238, -0.0334,\n",
       "        -0.1900, -0.2796, -0.1100, -0.1270, -0.0018, -0.0813,  0.0889, -0.1201,\n",
       "        -0.0937, -0.0096, -0.1183, -0.1321, -0.0857,  0.0891,  0.2931, -0.0423,\n",
       "         0.2086,  0.5333, -0.2142, -0.2307, -0.3187, -0.0775, -0.1164, -0.2546,\n",
       "         0.0318, -0.0810,  0.1116,  0.1264, -0.1230, -0.2088,  0.0108,  0.4909,\n",
       "         0.1728, -0.1331, -0.0080, -0.3447,  0.5024, -0.0092, -0.0625, -0.1091,\n",
       "        -0.1239, -0.1336,  0.2848, -0.1123])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors_fasttext['maison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c995fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 111529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "#word_dict = []\n",
    "#with open(data_path, 'r', encoding='utf-8') as f: \n",
    " #   for line in f:\n",
    "        #values = line.split(' ')\n",
    "        #line = re.split(r'fr/',line)\n",
    "        #values = re.split(\" \", line[1])\n",
    "word = pretrained_vectors_fasttext.stoi.keys()#[12:]\n",
    "        #values = pretrained_vectors_fasttext.stoi.values()\n",
    "        #word_dict.append(word) \n",
    "for word in word_counts:\n",
    "    embedding = pretrained_vectors_fasttext[word]\n",
    "            #embedding = np.asarray(values[1:])#, dtype='float32'\n",
    "    embeddings_index[word] = embedding\n",
    "        #print(word)\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3fd1780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from fastText: 0\n",
      "Percent of words that are missing from vocabulary: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from fastText:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a09862a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 111529\n",
      "Number of words we will use: 111533\n",
      "Percent of words we will use: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in CN\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe9464a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111533\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "070ec8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5b725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 5339165\n",
      "Total number of UNKs in headlines: 0\n",
      "Percent of words that are UNK: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_target and clean_input\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_target, word_count, unk_count = convert_to_ints(clean_target, word_count, unk_count)\n",
    "int_input, word_count, unk_count = convert_to_ints(clean_input, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4d7ba733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba3497ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean      34.393112\n",
      "std       12.316238\n",
      "min        3.000000\n",
      "25%       26.000000\n",
      "50%       34.000000\n",
      "75%       42.000000\n",
      "max      164.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  21401.000000\n",
      "mean     216.088921\n",
      "std      106.650884\n",
      "min       11.000000\n",
      "25%      142.000000\n",
      "50%      192.000000\n",
      "75%      260.000000\n",
      "max     1884.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_target = create_lengths(int_target)\n",
    "lengths_input = create_lengths(int_input)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_target.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_input.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f3bde881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.0\n",
      "417.0\n",
      "525.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"input\"\n",
    "print(np.percentile(lengths_input.counts, 90))\n",
    "print(np.percentile(lengths_input.counts, 95))\n",
    "print(np.percentile(lengths_input.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c28e724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n",
      "54.0\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of \"target\"\n",
    "print(np.percentile(lengths_target.counts, 90))\n",
    "print(np.percentile(lengths_target.counts, 95))\n",
    "print(np.percentile(lengths_target.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9519aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1ea53d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19058\n",
      "19058\n"
     ]
    }
   ],
   "source": [
    "sorted_target = []\n",
    "sorted_input = []\n",
    "max_input_length = 377\n",
    "max_target_length = 70\n",
    "min_length = 2\n",
    "unk_input_limit = 1\n",
    "unk_target_limit = 0\n",
    "\n",
    "for length in range(min(lengths_input.counts), max_input_length): \n",
    "    for count, words in enumerate(int_target):\n",
    "        if (len(int_target[count]) >= min_length and\n",
    "            len(int_target[count]) <= max_target_length and\n",
    "            len(int_input[count]) >= min_length and\n",
    "            unk_counter(int_target[count]) <= unk_target_limit and\n",
    "            unk_counter(int_input[count]) <= unk_input_limit and\n",
    "            length == len(int_input[count])\n",
    "           ):\n",
    "            sorted_target.append(int_target[count])\n",
    "            sorted_input.append(int_input[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_target))\n",
    "print(len(sorted_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddcfd9",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25ce54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 150, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 150, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Your max_length is set to 150, but your input_length is only 139. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=69)\n",
      "Your max_length is set to 150, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 150, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Your max_length is set to 150, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      "Your max_length is set to 150, but your input_length is only 146. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      "Your max_length is set to 150, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 150, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 150, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 150, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Your max_length is set to 150, but your input_length is only 118. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=59)\n",
      "Your max_length is set to 150, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "Your max_length is set to 150, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Your max_length is set to 150, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      "Your max_length is set to 150, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Your max_length is set to 150, but your input_length is only 53. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 150, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 150, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
      "Your max_length is set to 150, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 150, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Your max_length is set to 150, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Your max_length is set to 150, but your input_length is only 73. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "Your max_length is set to 150, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 150, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 150, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Your max_length is set to 150, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 150, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def summarize_article(content):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\")\n",
    "    max_tokens = 1024  # Ajustez selon la longueur maximale du modèle\n",
    "    chunks = [content[i:i + max_tokens] for i in range(0, len(content), max_tokens)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summarize_and_store_articles():\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"StageTT\"]\n",
    "    collection = db[\"TT\"]\n",
    "\n",
    "    articles = list(collection.find({}))\n",
    "    for article in articles:\n",
    "        content = article.get('Content')\n",
    "        if content:\n",
    "            summary = summarize_article(content)\n",
    "            collection.update_one(\n",
    "                {\"_id\": article[\"_id\"]},\n",
    "                {\"$set\": {\"Resume\": summary}},\n",
    "                upsert=True\n",
    "            )\n",
    "    print(\"Résumé des articles stockés dans la collection 'TT'.\")\n",
    "\n",
    "\n",
    "summarize_and_store_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ab178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
